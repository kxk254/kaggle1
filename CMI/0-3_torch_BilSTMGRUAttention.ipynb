{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79b6d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json, joblib\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82880947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ imports ready · torch 2.7.1+cpu device : cpu\n"
     ]
    }
   ],
   "source": [
    "# (Competition metric will only be imported when TRAINing)\n",
    "TRAIN = True                     # ← set to True when you want to train\n",
    "RAW_DIR = Path(\"C:\\\\Users\\\\konno\\\\SynologyDrive\\\\datasciense\\\\projects_foler\\\\1_kaggle\\\\CMI\\\\cmi-detect-behavior-with-sensor-data\")\n",
    "\n",
    "# used when TRAIN=False\n",
    "PRETRAINED_DIR = Path(\"C:\\\\Users\\\\konno\\\\SynologyDrive\\\\datasciense\\\\projects_foler\\\\1_kaggle\\\\CMI\\\\cmi-detect-behavior-with-sensor-data\\\\pretrained-model\")\n",
    "EXPORT_DIR = PRETRAINED_DIR                                  # artefacts will be saved here\n",
    "BATCH_SIZE = 16  #64\n",
    "PAD_PERCENTILE = 95\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "MIXUP_ALPHA = 0.4\n",
    "EPOCHS = 4 #160\n",
    "PATIENCE = 40\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "print(\"▶ imports ready · torch\", torch.__version__, \"device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "179bbdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d535f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_sum(x):\n",
    "    return x.sum(dim=1)\n",
    "\n",
    "def squeeze_last_axis(x):\n",
    "    return x.squeeze(-1)\n",
    "\n",
    "def expand_last_axis(x):\n",
    "    return x.unsqueeze(-1)\n",
    "\n",
    "class SEBlock1D(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, time)\n",
    "        b, c, _ = x.size()\n",
    "        y = self.global_avg_pool(x).view(b, c)\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = torch.sigmoid(self.fc2(y)).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ResidualSEBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, pool_size=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.se = SEBlock1D(out_channels)\n",
    "        \n",
    "        self.match_channels = None\n",
    "        if in_channels != out_channels:\n",
    "            self.match_channels = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, padding=0, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.se(out)\n",
    "\n",
    "        if self.match_channels is not None:\n",
    "            identity = self.match_channels(identity)\n",
    "        \n",
    "        out = F.relu(out + identity)\n",
    "        out = self.pool(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.attn_fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, time, features)\n",
    "        # Compute scores with tanh activation\n",
    "        scores = torch.tanh(self.attn_fc(x))  # (batch, time, 1)\n",
    "        scores = scores.squeeze(-1)           # (batch, time)\n",
    "\n",
    "        # Softmax over time dimension to get weights\n",
    "        weights = F.softmax(scores, dim=1)    # (batch, time)\n",
    "        weights = weights.unsqueeze(-1)       # (batch, time, 1)\n",
    "\n",
    "        # Weighted sum of input features over time\n",
    "        context = (x * weights).sum(dim=1)    # (batch, features)\n",
    "        return context\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7803102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list[str], scaler: StandardScaler):\n",
    "    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n",
    "    return scaler.transform(mat).astype('float32')\n",
    "\n",
    "# MixUp the data argumentation in order to regularize the neural network. \n",
    "\n",
    "class MixupGenerator(Dataset):\n",
    "    def __init__(self, X, y, alpha=0.2):\n",
    "        \"\"\"  \n",
    "        X: np.array or totch.Tensor of shape (N, ...)\n",
    "        y: np.array or torch.Tensor of shape (N, ...)\n",
    "        alpha: Beta distribution parameter for mixup\n",
    "        \"\"\"\n",
    "        self.X = torch.tensor(X, dtype=torch.float32) if isinstance(X, np.ndarray) else X\n",
    "        self.y = torch.tensor(y, dtype=torch.float32) if isinstance(y, np.ndarray) else y\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get current sample\n",
    "        x1, y1 = self.X[index], self.y[index]\n",
    "\n",
    "        # Choose a random other sample\n",
    "        mix_index = np.random.randint(0, len(self.X))\n",
    "        x2, y2 = self.X[mix_index], self.y[mix_index]\n",
    "\n",
    "        # Mix them\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        x_mix = lam * x1 + (1 - lam) * x2\n",
    "        y_mix = lam * y1 + (1 - lam) * y2\n",
    "\n",
    "        return x_mix, y_mix\n",
    "    \n",
    "# dataset = MixupGenerator(X_train, y_train, alpha=0.2)\n",
    "# loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "def pad_or_truncate(seq, max_len, mode=TRAIN, pad_value=0.0, dtype=np.float32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pads or truncates a sequence to a fixed length.\n",
    "\n",
    "    Parameters:\n",
    "    - seq: np.ndarray of shape (L, D)\n",
    "    - max_len: int, desired sequence length\n",
    "    - mode: bool, True = random pad, False = regular pad\n",
    "    - pad_value: float or int, value to use for padding\n",
    "    - dtype: np.dtype, dtype for the output array\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray of shape (max_len, D)\n",
    "    \"\"\"\n",
    "    # print(\"sequence shape\", seq.shape)\n",
    "    L, D = seq.shape\n",
    "    # print(\"mode = \", mode)\n",
    "\n",
    "    if L > max_len:\n",
    "        return seq[:max_len] # truncate if too long\n",
    "\n",
    "    elif L < max_len:\n",
    "        total_padding = max_len - L\n",
    "        \n",
    "        if mode:\n",
    "            pad_start = np.random.randint(0, total_padding + 1)\n",
    "            pad_end = total_padding - pad_start\n",
    "            \n",
    "        else:\n",
    "            pad_start = 0\n",
    "            pad_end = total_padding\n",
    "\n",
    "        start_padding = np.full((pad_start, D), pad_value, dtype=dtype)\n",
    "        end_padding = np.full((pad_end, D), pad_value, dtype=dtype)\n",
    "        padded = np.vstack((start_padding, seq, end_padding))\n",
    "        # print(\"padded shape\", padded.shape)\n",
    "        return padded\n",
    "\n",
    "    else:\n",
    "        return seq.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0242b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    \n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "             \n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "\n",
    "            # Calculate the relative rotation\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            \n",
    "            # Convert delta rotation to angular velocity vector\n",
    "            # The rotation vector (Euler axis * angle) scaled by 1/dt\n",
    "            # is a good approximation for small delta_rot\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            # If quaternion is invalid, angular velocity remains zero\n",
    "            pass\n",
    "            \n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0 # Или np.nan, в зависимости от желаемого поведения\n",
    "            continue\n",
    "        try:\n",
    "            # Преобразование кватернионов в объекты Rotation\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "\n",
    "            # Вычисление углового расстояния: 2 * arccos(|real(p * q*)|)\n",
    "            # где p* - сопряженный кватернион q\n",
    "            # В scipy.spatial.transform.Rotation, r1.inv() * r2 дает относительное вращение.\n",
    "            # Угол этого относительного вращения - это и есть угловое расстояние.\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            \n",
    "            # Угол rotation vector соответствует угловому расстоянию\n",
    "            # Норма rotation vector - это угол в радианах\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 # В случае недействительных кватернионов\n",
    "            pass\n",
    "            \n",
    "    return angular_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe10b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoBranchGestureModel(nn.Module):\n",
    "    def __init__(self, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "        super().__init__()\n",
    "        self.imu_dim = imu_dim\n",
    "        self.tof_dim = tof_dim\n",
    "\n",
    "       \n",
    "        # IMU deep branch\n",
    "        self.imu_branch = nn.Sequential(\n",
    "            ResidualSEBlock1D(imu_dim, 64, kernel_size=3, dropout=0.1),\n",
    "            ResidualSEBlock1D(64, 128, kernel_size=5, dropout=0.1)\n",
    "        )\n",
    "\n",
    "        # TOF Lighter branch\n",
    "        self.tof_branch = nn.Sequential(\n",
    "            nn.Conv1d(tof_dim, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),  #ReLU\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(256, 128, batch_first=True, bidirectional=True)\n",
    "        self.gru = nn.GRU(256, 128, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Gaussian noise (manual) and projection\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Dropout(0.09),\n",
    "            nn.Linear(256, 16),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.pre_attn_dropout = nn.Dropout(0.4)\n",
    "        self.attn_rnn = AttentionLayer(128*2 + 128*2 + 16)\n",
    "        self.attn_trans = AttentionLayer(256)  # transformer dim\n",
    "\n",
    "        # Dense layer\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(784, 256, bias=False),  # 524 - 784\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),  #ReLU\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128, bias=False),\n",
    "            nn.GELU(),   #ReLU\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, n_classes),  # Softmax handled by loss (e.g., CrossEntropyLoss)\n",
    "        )\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model=256)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, dropout=0.1)  #, norm_first=True\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        imu = x[:, :, :self.imu_dim].permute(0, 2, 1) #(B, imu_dim, T)\n",
    "        tof = x[: ,:, self.imu_dim:].permute(0, 2, 1) #(B, tof_dim, T)\n",
    "\n",
    "        imu_feat = self.imu_branch(imu)\n",
    "        tof_feat = self.tof_branch(tof)\n",
    "\n",
    "        imu_feat = imu_feat.permute(0, 2, 1)\n",
    "        tof_feat = tof_feat.permute(0, 2, 1)\n",
    "\n",
    "        merged = torch.cat([imu_feat, tof_feat], dim=-1)  #(B, T', 256)\n",
    "\n",
    "        # ---- LSTM & GRU Path\n",
    "        xa, _ = self.lstm(merged)\n",
    "        xb, _ = self.gru(merged)\n",
    "        xc = self.projection(merged)\n",
    "\n",
    "        x_cat = torch.cat([xa, xb, xc], dim=-1)  #(B, T', 512)\n",
    "        x_cat = self.pre_attn_dropout(x_cat)\n",
    "        context_rnn = self.attn_rnn(x_cat)\n",
    "\n",
    "        #  --- Transformer Path\n",
    "        merged_pe = self.positional_encoding(merged)\n",
    "        x_trans = self.transformer(merged_pe.permute(1, 0 ,2))  # (T', B, 256)\n",
    "        x_trans = x_trans.permute(1, 0, 2)  #(B, T', 256)\n",
    "        context_trans = self.attn_trans(x_trans)\n",
    "\n",
    "        final_context = torch.cat([context_rnn, context_trans], dim=-1)  #(B, 784)\n",
    "\n",
    "        return self.mlp(final_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b507c241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ TRAIN MODE – loading dataset …\n",
      " Calculating base engineered IMU features (magnitude, angle) ...\n",
      " Calculating engineered IMU derivatives (jerk, angular velocity) for original acc_mag ...\n",
      " Removing gravity and calculating linear acceleration features...\n",
      "  Calculating angular velocity from quaternion derivatives...\n",
      "  Calculating angular distance between successive quaternions...\n",
      "  IMU (incl. engineered & derivatives) 17 | THM + Aggregated TOF 25 | total 42 features\n",
      "  Building sequences with aggregated TOF and preparing data for scaler...\n",
      "  Fitting StandardScaler...\n",
      "  Scaling and padding sequences...\n",
      "  Splitting data and preparing for training...\n"
     ]
    }
   ],
   "source": [
    "### DATA CREATION\n",
    "print(\"▶ TRAIN MODE – loading dataset …\")\n",
    "df = pd.read_csv(RAW_DIR / \"train.csv\")\n",
    "\n",
    "train_dem_df = pd.read_csv(RAW_DIR / \"train_demographics.csv\")\n",
    "df_for_groups = pd.merge(df.copy(), train_dem_df, on='subject', how='left')\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "np.save(EXPORT_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "gesture_classes = le.classes_\n",
    "\n",
    "print(\" Calculating base engineered IMU features (magnitude, angle) ...\")\n",
    "df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "df['rot_angle'] = 2* np.arccos(df['rot_w'].clip(-1, 1))\n",
    "\n",
    "print(\" Calculating engineered IMU derivatives (jerk, angular velocity) for original acc_mag ...\")\n",
    "df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "\n",
    "print(\" Removing gravity and calculating linear acceleration features...\")\n",
    "\n",
    "linear_accel_list = []\n",
    "for _, group in df.groupby('sequence_id'):\n",
    "    acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n",
    "    rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "    linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
    "    linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n",
    "\n",
    "df_linear_accel = pd.concat(linear_accel_list)\n",
    "df = pd.concat([df, df_linear_accel], axis=1)\n",
    "\n",
    "df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "\n",
    "print(\"  Calculating angular velocity from quaternion derivatives...\")\n",
    "angular_vel_list = []\n",
    "for _, group in df.groupby('sequence_id'):\n",
    "    rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "    angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
    "    angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n",
    "\n",
    "df_angular_vel = pd.concat(angular_vel_list)\n",
    "df = pd.concat([df, df_angular_vel], axis=1)\n",
    "\n",
    "print(\"  Calculating angular distance between successive quaternions...\")\n",
    "angular_distance_list = []\n",
    "for _, group in df.groupby('sequence_id'):\n",
    "    rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "    angular_dist_group = calculate_angular_distance(rot_data_group)\n",
    "    angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n",
    "\n",
    "df_angular_distance = pd.concat(angular_distance_list)\n",
    "df = pd.concat([df, df_angular_distance], axis=1)\n",
    "\n",
    "meta_cols = { } # This was an empty dict in your provided code, keeping it as is.\n",
    "\n",
    "imu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n",
    "imu_cols_base.extend([c for c in df.columns if c.startswith('rot_') and c not in ['rot_angle', 'rot_angle_vel']])\n",
    "\n",
    "imu_engineered_features = [\n",
    "    'acc_mag', 'rot_angle',\n",
    "    'acc_mag_jerk', 'rot_angle_vel',\n",
    "    'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "    'angular_vel_x', 'angular_vel_y', 'angular_vel_z', # Existing new features\n",
    "    'angular_distance' # Added new feature\n",
    "]\n",
    "imu_cols = imu_cols_base + imu_engineered_features\n",
    "imu_cols = list(dict.fromkeys(imu_cols)) # Для удаления дубликатов\n",
    "\n",
    "thm_cols_original = [c for c in df.columns if c.startswith('thm_')]\n",
    "\n",
    "## tof data\n",
    "tof_aggregated_cols_template = []\n",
    "for i in range(1, 6):\n",
    "    tof_aggregated_cols_template.extend([f'tof_{i}_mean', f'tof_{i}_std', f'tof_{i}_min', f'tof_{i}_max'])\n",
    "\n",
    "final_feature_cols = imu_cols + thm_cols_original + tof_aggregated_cols_template\n",
    "imu_dim_final = len(imu_cols)\n",
    "tof_thm_aggregated_dim_final = len(thm_cols_original) + len(tof_aggregated_cols_template)\n",
    "\n",
    "print(f\"  IMU (incl. engineered & derivatives) {imu_dim_final} | THM + Aggregated TOF {tof_thm_aggregated_dim_final} | total {len(final_feature_cols)} features\")\n",
    "np.save(EXPORT_DIR / \"feature_cols.npy\", np.array(final_feature_cols))\n",
    "\n",
    "print(\"  Building sequences with aggregated TOF and preparing data for scaler...\")\n",
    "seq_gp = df.groupby('sequence_id') \n",
    "\n",
    "all_steps_for_scaler_list = []\n",
    "X_list_unscaled, y_list_int_for_stratify, lens = [], [], [] \n",
    "\n",
    "for seq_id, seq_df_orig in seq_gp:\n",
    "    seq_df = seq_df_orig.copy()\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        pixel_cols_tof = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "        tof_sensor_data = seq_df[pixel_cols_tof].replace(-1, np.nan)\n",
    "        seq_df[f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n",
    "        seq_df[f'tof_{i}_std']  = tof_sensor_data.std(axis=1)\n",
    "        seq_df[f'tof_{i}_min']  = tof_sensor_data.min(axis=1)\n",
    "        seq_df[f'tof_{i}_max']  = tof_sensor_data.max(axis=1)\n",
    "    \n",
    "    mat_unscaled = seq_df[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    \n",
    "    all_steps_for_scaler_list.append(mat_unscaled)\n",
    "    X_list_unscaled.append(mat_unscaled)\n",
    "    y_list_int_for_stratify.append(seq_df['gesture_int'].iloc[0])\n",
    "    lens.append(len(mat_unscaled))\n",
    "\n",
    "# fit scaler\n",
    "print(\"  Fitting StandardScaler...\")\n",
    "all_steps_concatenated = np.concatenate(all_steps_for_scaler_list, axis=0)\n",
    "scaler = StandardScaler().fit(all_steps_concatenated)\n",
    "joblib.dump(scaler, EXPORT_DIR / \"scaler.pkl\")\n",
    "del all_steps_for_scaler_list, all_steps_concatenated\n",
    "\n",
    "# scale individual sequences\n",
    "print(\"  Scaling and padding sequences...\")\n",
    "X_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\n",
    "del X_list_unscaled\n",
    "\n",
    "# calculate pad length\n",
    "pad_len = int(np.percentile(lens, PAD_PERCENTILE))\n",
    "np.save(EXPORT_DIR / \"sequence_maxlen.npy\", pad_len)\n",
    "\n",
    "# padding section\n",
    "X_padded_np = np.stack([pad_or_truncate(seq, pad_len) for seq in X_scaled_list])\n",
    "X = torch.tensor(X_padded_np, dtype=torch.float32)  # shape: (N, T, D)\n",
    "del X_scaled_list\n",
    "\n",
    "y_int_for_stratify = np.array(y_list_int_for_stratify)\n",
    "y = F.one_hot(torch.tensor(y_int_for_stratify), num_classes=len(le.classes_)).float()\n",
    "\n",
    "print(\"  Splitting data and preparing for training...\")\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=82, stratify=y_int_for_stratify)\n",
    "\n",
    "cw_vals = compute_class_weight('balanced', classes=np.arange(len(le.classes_)), y=y_int_for_stratify)\n",
    "\n",
    "# Data Loader\n",
    "train_dataset = MixupGenerator(X_tr, y_tr, alpha=0.2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = MixupGenerator(X_val, y_val, alpha=0.2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f95d3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/408 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of xb :torch.Size([16, 127, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [01:20<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Val Acc: 0.4390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [01:54<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Val Acc: 0.5034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [01:48<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Val Acc: 0.5181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [01:53<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Val Acc: 0.5677\n"
     ]
    }
   ],
   "source": [
    "print(\"⏩ model created .....\")\n",
    "model = TwoBranchGestureModel(imu_dim=imu_dim_final, tof_dim=tof_thm_aggregated_dim_final, n_classes=len(le.classes_))\n",
    "model.to(device)\n",
    "\n",
    "print(\"⏩ training started .....\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR_INIT, weight_decay=WD)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)  # T_max = number of epochs\n",
    "weights_tensor = torch.tensor(cw_vals, dtype=torch.float32).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights_tensor, label_smoothing=0.1)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience, patience_counter = 10, 0\n",
    "EPOCHS = EPOCHS\n",
    "counter = 0\n",
    "ud = []\n",
    "steps = []\n",
    "lrs = []\n",
    "\n",
    "print(\"✅ Epoch starts .....\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0         # <-- reset here\n",
    "    total = 0           # <-- reset here\n",
    "    for xb, yb in tqdm.tqdm(train_loader):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        if counter == 0:\n",
    "            print(f\"shape of xb :{xb.shape}\")\n",
    "            counter +=1\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # optional\n",
    "        # Track parameter-wise update dynamixs\n",
    "        with torch.no_grad():\n",
    "            updates = []\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None and p.data.stad() > 0:\n",
    "                    ratio = (LR_INIT * p.grad.std() / (p.data.std() + 1e-8))\n",
    "                    updates.append(ratio.log10().item())  # log10 for readability\n",
    "                    \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # accumulate train data\n",
    "        total_loss += loss.item()\n",
    "        lrs.append(scheduler.get_last_lr()[0])\n",
    "        steps.append(epoch * BATCH_SIZE + counter + 1)\n",
    "\n",
    "        pred_labels = preds.argmax(dim=1)\n",
    "        correct += (pred_labels == yb).sum().item()\n",
    "        total += yb.size()\n",
    "    \n",
    "    train_acc = correct / total\n",
    "    print(f\"Eposh {epoch} | Train Loss: {total_loss / len(train_loader):.4f} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            pred_labels = preds.argmax(1)\n",
    "            true_labels = yb.argmax(1)\n",
    "            correct += (pred_labels == true_labels).sum().item()\n",
    "            total += yb.size(0)\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), EXPORT_DIR / \"best_model.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualize UD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flatten and average across steps for each parameter group\n",
    "ud_tensor = torch.tensor(ud)  # shape: (steps, n_param_groups)\n",
    "avg_updates = ud_tensor.mean(dim=0)\n",
    "\n",
    "plt.bar(range(len(avg_updates)), avg_updates)\n",
    "plt.title(\"Avg log10(update/weight std) per parameter tensor\")\n",
    "plt.xlabel(\"Param Group Index\")\n",
    "plt.ylabel(\"log10(update/weight)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fca264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-out H-F1 = 0.3496\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(EXPORT_DIR / \"best_model.pt\"))\n",
    "model.eval()\n",
    "preds_val = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, _ in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        preds_val.append(logits.argmax(1).cpu())\n",
    "\n",
    "preds_val = torch.cat(preds_val).numpy()\n",
    "true_val_int = y_val.argmax(1).numpy()\n",
    "\n",
    "# Evaluation\n",
    "from cmi_2025_metric_copy_for_import import CompetitionMetric\n",
    "\n",
    "h_f1 = CompetitionMetric().calculate_hierarchical_f1(\n",
    "    pd.DataFrame({'gesture': le.classes_[true_val_int]}),\n",
    "    pd.DataFrame({'gesture': le.classes_[preds_val]})\n",
    ")\n",
    "print(\"Hold-out H-F1 =\", round(h_f1, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
