{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5571e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc6d0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ imports ready · pytorch 2.7.1+cu128 · device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "import random, math\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from scipy.signal import firwin\n",
    "from cmi_2025_metric_copy_for_import import CompetitionMetric\n",
    "\n",
    "import polars as pl\n",
    "# Configuration\n",
    "TRAIN = True                     # ← set to True when you want to train\n",
    "RAW_DIR = Path(\"C:/Users/konno/SynologyDrive/datasciense/projects_foler/1_kaggle/CMI/cmi-detect-behavior-with-sensor-data\")\n",
    "PRETRAINED_DIR = Path(\"C:/Users/konno/SynologyDrive/datasciense/projects_foler/1_kaggle/CMI/cmi-detect-behavior-with-sensor-data\") # used when TRAIN=False\n",
    "EXPORT_DIR = Path(\"C:/Users/konno/SynologyDrive/datasciense/projects_foler/1_kaggle/CMI/cmi-detect-behavior-with-sensor-data/CMI3_pytorch\")                                    # artefacts will be saved here\n",
    "BATCH_SIZE = 64\n",
    "PAD_PERCENTILE = 125 #100\n",
    "EPOCHS = 125\n",
    "maxlen = PAD_PERCENTILE\n",
    "LR_INIT = 1e-3\n",
    "WD = 3e-3\n",
    "# MIXUP_ALPHA = 0.4\n",
    "PATIENCE = 40\n",
    "FOLDS = 5\n",
    "random_state = 42\n",
    "epochs_warmup = 20\n",
    "warmup_lr_init = 1.822126131809773e-05\n",
    "lr_min = 3.810323058740104e-09\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"▶ imports ready · pytorch {torch.__version__} · device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aae7d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Model Components\n",
    "# ================================\n",
    "mean = torch.tensor([\n",
    "    0,  0, 0, 0, 0,\n",
    "    0,  9.0319e-03,  1.0849e+00, -2.6186e-03,  3.7651e-03,\n",
    "    -5.3660e-03, -2.8177e-03,  1.3318e-03, -1.5876e-04,  6.3495e-01,\n",
    "     6.2877e-01,  6.0607e-01,  6.2142e-01,  6.3808e-01,  6.5420e-01,\n",
    "     7.4102e-03, -3.4159e-03, -7.5237e-03, -2.6034e-02,  2.9704e-02,\n",
    "    -3.1546e-02, -2.0610e-03, -4.6986e-03, -4.7216e-03, -2.6281e-02,\n",
    "     1.5799e-02,  1.0016e-02\n",
    "], dtype=torch.float32).view(1, -1, 1).to(device)         \n",
    "\n",
    "std = torch.tensor([\n",
    "    1, 1, 1, 1, 1, 1, 0.2067, 0.8583, 0.3162,\n",
    "    0.2668, 0.2917, 0.2341, 0.3023, 0.3281, 1.0264, 0.8838, 0.8686, 1.0973,\n",
    "    1.0267, 0.9018, 0.4658, 0.2009, 0.2057, 1.2240, 0.9535, 0.6655, 0.2941,\n",
    "    0.3421, 0.8156, 0.6565, 1.1034, 1.5577\n",
    "], dtype=torch.float32).view(1, -1, 1).to(device) + 1e-8  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aad5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImuFeatureExtractor(nn.Module):\n",
    "    def __init__(self, fs=100., add_quaternion=False):\n",
    "        super().__init__()\n",
    "        self.fs = fs\n",
    "        self.add_quaternion = add_quaternion\n",
    "\n",
    "        k = 15\n",
    "        self.lpf = nn.Conv1d(6, 6, kernel_size=k, padding=k//2,\n",
    "                             groups=6, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.lpf.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.lpf_acc  = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n",
    "        self.lpf_gyro = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n",
    "\n",
    "    def forward(self, imu):\n",
    "        # imu: \n",
    "        B, C, T = imu.shape\n",
    "        acc  = imu[:, 0:3, :]                 # acc_x, acc_y, acc_z\n",
    "        gyro = imu[:, 3:6, :]                 # gyro_x, gyro_y, gyro_z\n",
    "        extra = imu[:, 6:, :]                 \n",
    "\n",
    "        # 1) magnitude\n",
    "        acc_mag  = torch.norm(acc,  dim=1, keepdim=True)          # (B,1,T)\n",
    "        gyro_mag = torch.norm(gyro, dim=1, keepdim=True)\n",
    "\n",
    "        # 2) jerk \n",
    "        jerk = F.pad(acc[:, :, 1:] - acc[:, :, :-1], (1,0))       # (B,3,T)\n",
    "        gyro_delta = F.pad(gyro[:, :, 1:] - gyro[:, :, :-1], (1,0))\n",
    "\n",
    "        # 3) energy\n",
    "        acc_pow  = acc ** 2\n",
    "        gyro_pow = gyro ** 2\n",
    "\n",
    "        # 4) LPF / HPF \n",
    "        acc_lpf  = self.lpf_acc(acc)\n",
    "        acc_hpf  = acc - acc_lpf\n",
    "        gyro_lpf = self.lpf_gyro(gyro)\n",
    "        gyro_hpf = gyro - gyro_lpf\n",
    "\n",
    "        features = [\n",
    "            acc, gyro,\n",
    "            acc_mag, gyro_mag,\n",
    "            jerk, gyro_delta,\n",
    "            acc_pow, gyro_pow,\n",
    "            acc_lpf, acc_hpf,\n",
    "            gyro_lpf, gyro_hpf,\n",
    "        ]\n",
    "        return torch.cat(features, dim=1)  # (B, C_out, T)\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ResidualSECNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, pool_size=2, dropout=0.3, weight_decay=1e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First conv block\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Second conv block\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # SE block\n",
    "        self.se = SEBlock(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        \n",
    "        # First conv\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Second conv\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # SE block\n",
    "        out = self.se(out)\n",
    "        \n",
    "        # Add shortcut\n",
    "        out += shortcut\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Pool and dropout\n",
    "        out = self.pool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, hidden_dim)\n",
    "        scores = torch.tanh(self.attention(x))  # (batch, seq_len, 1)\n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1)  # (batch, seq_len)\n",
    "        context = torch.sum(x * weights.unsqueeze(-1), dim=1)  # (batch, hidden_dim)\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TwoBranchModel(nn.Module):\n",
    "    def __init__(self, pad_len, imu_dim_raw, tof_dim, n_classes, dropouts=[0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.3], feature_engineering=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.feature_engineering = feature_engineering\n",
    "        if feature_engineering:\n",
    "            self.imu_fe = ImuFeatureExtractor(**kwargs)\n",
    "            imu_dim = 32            \n",
    "        else:\n",
    "            self.imu_fe = nn.Identity()\n",
    "            imu_dim = imu_dim_raw   \n",
    "            \n",
    "        self.imu_dim = imu_dim\n",
    "        self.tof_dim = tof_dim\n",
    "\n",
    "        self.fir_nchan = 7\n",
    "\n",
    "        weight_decay = 3e-3\n",
    "\n",
    "        numtaps = 33  \n",
    "        fir_coef = firwin(numtaps, cutoff=1.0, fs=10.0, pass_zero=False)\n",
    "        fir_kernel = torch.tensor(fir_coef, dtype=torch.float32).view(1, 1, -1)\n",
    "        fir_kernel = fir_kernel.repeat(7, 1, 1)  # (imu_dim, 1, numtaps)\n",
    "        self.register_buffer(\"fir_kernel\", fir_kernel)\n",
    "        \n",
    "        # IMU deep branch\n",
    "        self.imu_block1 = ResidualSECNNBlock(imu_dim, 64, 3, dropout=dropouts[0], weight_decay=weight_decay)\n",
    "        self.imu_block2 = ResidualSECNNBlock(64, 128, 5, dropout=dropouts[1], weight_decay=weight_decay)\n",
    "        \n",
    "        # TOF/Thermal lighter branch\n",
    "        self.tof_conv1 = nn.Conv1d(tof_dim, 64, 3, padding=1, bias=False)\n",
    "        self.tof_bn1 = nn.BatchNorm1d(64)\n",
    "        self.tof_pool1 = nn.MaxPool1d(2)\n",
    "        self.tof_drop1 = nn.Dropout(dropouts[2])\n",
    "        \n",
    "        self.tof_conv2 = nn.Conv1d(64, 128, 3, padding=1, bias=False)\n",
    "        self.tof_bn2 = nn.BatchNorm1d(128)\n",
    "        self.tof_pool2 = nn.MaxPool1d(2)\n",
    "        self.tof_drop2 = nn.Dropout(dropouts[3])\n",
    "        \n",
    "        # BiLSTM\n",
    "        self.bilstm = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropouts[4])\n",
    "        \n",
    "        # Attention\n",
    "        self.attention = AttentionLayer(256)  # 128*2 for bidirectional\n",
    "        \n",
    "        # Dense layers\n",
    "        self.dense1 = nn.Linear(256, 256, bias=False)\n",
    "        self.bn_dense1 = nn.BatchNorm1d(256)\n",
    "        self.drop1 = nn.Dropout(dropouts[5])\n",
    "        \n",
    "        self.dense2 = nn.Linear(256, 128, bias=False)\n",
    "        self.bn_dense2 = nn.BatchNorm1d(128)\n",
    "        self.drop2 = nn.Dropout(dropouts[6])\n",
    "        \n",
    "        self.classifier = nn.Linear(128, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input\n",
    "        \n",
    "        imu = x[:, :, :self.fir_nchan].transpose(1, 2)  # (batch, imu_dim, seq_len)\n",
    "        tof = x[:, :, self.fir_nchan:].transpose(1, 2)  # (batch, tof_dim, seq_len)\n",
    "\n",
    "        imu = self.imu_fe(imu)   # (B, imu_dim, T)\n",
    "        filtered = F.conv1d(\n",
    "            imu[:, :self.fir_nchan, :],        # (B,7,T)\n",
    "            self.fir_kernel,\n",
    "            padding=self.fir_kernel.shape[-1] // 2,\n",
    "            groups=self.fir_nchan,\n",
    "        )\n",
    "        \n",
    "        imu = torch.cat([filtered, imu[:, self.fir_nchan:, :]], dim=1)  \n",
    "        imu = (imu - mean) / std \n",
    "        # IMU branch\n",
    "        x1 = self.imu_block1(imu)\n",
    "        x1 = self.imu_block2(x1)\n",
    "        \n",
    "        # TOF branch\n",
    "        x2 = F.relu(self.tof_bn1(self.tof_conv1(tof)))\n",
    "        x2 = self.tof_drop1(self.tof_pool1(x2))\n",
    "        x2 = F.relu(self.tof_bn2(self.tof_conv2(x2)))\n",
    "        x2 = self.tof_drop2(self.tof_pool2(x2))\n",
    "        \n",
    "        # Concatenate branches\n",
    "        merged = torch.cat([x1, x2], dim=1).transpose(1, 2)  # (batch, seq_len, 256)\n",
    "        \n",
    "        # BiLSTM\n",
    "        lstm_out, _ = self.bilstm(merged)\n",
    "        lstm_out = self.lstm_dropout(lstm_out)\n",
    "        \n",
    "        # Attention\n",
    "        attended = self.attention(lstm_out)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = F.relu(self.bn_dense1(self.dense1(attended)))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.bn_dense2(self.dense2(x)))\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        # Classification\n",
    "        logits = (self.classifier(x))\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1efb6807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# Data Handling\n",
    "# ================================\n",
    "\n",
    "def pad_sequences_torch(sequences, maxlen, padding='post', truncating='post', value=0.0):\n",
    "    \"\"\"PyTorch equivalent of Keras pad_sequences\"\"\"\n",
    "    result = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= maxlen:\n",
    "            if truncating == 'post':\n",
    "                seq = seq[:maxlen]\n",
    "            else:  # 'pre'\n",
    "                seq = seq[-maxlen:]\n",
    "        else:\n",
    "            pad_len = maxlen - len(seq)\n",
    "            if padding == 'post':\n",
    "                seq = np.concatenate([seq, np.full((pad_len, seq.shape[1]), value)])\n",
    "            else:  # 'pre'\n",
    "                seq = np.concatenate([np.full((pad_len, seq.shape[1]), value), seq])\n",
    "        result.append(seq)\n",
    "    return np.array(result, dtype=np.float32)\n",
    "\n",
    "def preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list, scaler: StandardScaler):\n",
    "    \"\"\"Normalizes and cleans the time series sequence\"\"\"\n",
    "    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n",
    "    return scaler.transform(mat).astype('float32')\n",
    "\n",
    "class CMI3Dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 X_list,\n",
    "                 y_list,\n",
    "                 maxlen,\n",
    "                 mode=\"train\",\n",
    "                 imu_dim=7,\n",
    "                 augment=None):\n",
    "        self.X_list = X_list\n",
    "        self.mode = mode\n",
    "        self.y_list = y_list\n",
    "        self.maxlen = maxlen\n",
    "        self.imu_dim = imu_dim     \n",
    "        self.augment = augment   \n",
    "\n",
    "    def pad_sequences_torch(self, seq, maxlen, padding='post', truncating='post', value=0.0):\n",
    "\n",
    "        if seq.shape[0] >= maxlen:\n",
    "            if truncating == 'post':\n",
    "                seq = seq[:maxlen]\n",
    "            else:  # 'pre'\n",
    "                seq = seq[-maxlen:]\n",
    "        else:\n",
    "            pad_len = maxlen - seq.shape[0]\n",
    "            if padding == 'post':\n",
    "                seq = np.concatenate([seq, np.full((pad_len, seq.shape[1]), value)])\n",
    "            else:  # 'pre'\n",
    "                seq = np.concatenate([np.full((pad_len, seq.shape[1]), value), seq])\n",
    "        return seq  \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        X = self.X_list[index]\n",
    "        y = self.y_list[index]\n",
    "\n",
    "        # ---------- (A)  Augmentation ----------\n",
    "        if self.mode == \"train\" and self.augment is not None:\n",
    "            X = self.augment(X, self.imu_dim)     \n",
    "\n",
    "        X = self.pad_sequences_torch(X, self.maxlen, 'pre', 'pre')\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_list)\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self, model):\n",
    "        self.backup = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "class Augment:\n",
    "    def __init__(self,\n",
    "                 p_jitter=0.8, sigma=0.02, scale_range=[0.9,1.1],\n",
    "                 p_dropout=0.3,\n",
    "                 p_moda=0.5,          \n",
    "                 drift_std=0.005,     \n",
    "                 drift_max=0.25):      \n",
    "        self.p_jitter  = p_jitter\n",
    "        self.sigma     = sigma\n",
    "        self.scale_min, self.scale_max = scale_range\n",
    "        self.p_dropout = p_dropout\n",
    "        self.p_moda    = p_moda\n",
    "        self.drift_std = drift_std\n",
    "        self.drift_max = drift_max\n",
    "\n",
    "    # ---------- Jitter & Scaling ----------\n",
    "    def jitter_scale(self, x: np.ndarray) -> np.ndarray:\n",
    "        noise  = np.random.randn(*x.shape) * self.sigma\n",
    "        scale  = np.random.uniform(self.scale_min,\n",
    "                                   self.scale_max,\n",
    "                                   size=(1, x.shape[1]))\n",
    "        return (x + noise) * scale\n",
    "\n",
    "    # ---------- Sensor Drop-out ----------\n",
    "    def sensor_dropout(self,\n",
    "                       x: np.ndarray,\n",
    "                       imu_dim: int) -> np.ndarray:\n",
    "\n",
    "        if random.random() < self.p_dropout:\n",
    "            x[:, imu_dim:] = 0.0\n",
    "        return x\n",
    "\n",
    "    def motion_drift(self, x: np.ndarray, imu_dim: int) -> np.ndarray:\n",
    "\n",
    "        T = x.shape[0]\n",
    "\n",
    "        drift = np.cumsum(\n",
    "            np.random.normal(scale=self.drift_std, size=(T, 1)),\n",
    "            axis=0\n",
    "        )\n",
    "        drift = np.clip(drift, -self.drift_max, self.drift_max)   \n",
    "\n",
    "        x[:, :6] += drift\n",
    "\n",
    "        if imu_dim > 6:\n",
    "            x[:, 6:imu_dim] += drift     \n",
    "        return x\n",
    "    \n",
    "    # ---------- master call ----------\n",
    "    def __call__(self,\n",
    "                 x: np.ndarray,\n",
    "                 imu_dim: int) -> np.ndarray:\n",
    "        if random.random() < self.p_jitter:\n",
    "            x = self.jitter_scale(x)\n",
    "\n",
    "        if random.random() < self.p_moda:\n",
    "            x = self.motion_drift(x, imu_dim)\n",
    "\n",
    "        x = self.sensor_dropout(x, imu_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c94adf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ TRAIN MODE – loading dataset …\n",
      "  IMU 7 | TOF/THM 325 | total 332 features\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Training Pipeline\n",
    "# ================================\n",
    "print(\"▶ TRAIN MODE – loading dataset …\")\n",
    "df = pd.read_csv(RAW_DIR / \"train.csv\")\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "np.save(EXPORT_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "\n",
    "# Feature list\n",
    "meta_cols = {'gesture', 'gesture_int', 'sequence_type', 'behavior', 'orientation',\n",
    "                'row_id', 'subject', 'phase', 'sequence_id', 'sequence_counter'}\n",
    "feature_cols = [c for c in df.columns if c not in meta_cols]\n",
    "\n",
    "imu_cols = [c for c in feature_cols if not (c.startswith('thm_') or c.startswith('tof_'))]\n",
    "tof_cols = [c for c in feature_cols if c.startswith('thm_') or c.startswith('tof_')]\n",
    "print(f\"  IMU {len(imu_cols)} | TOF/THM {len(tof_cols)} | total {len(feature_cols)} features\")\n",
    "\n",
    "# Global scaler\n",
    "scaler = StandardScaler().fit(df[feature_cols].ffill().bfill().fillna(0).values)\n",
    "joblib.dump(scaler, EXPORT_DIR / \"scaler.pkl\")\n",
    "\n",
    "# Build sequences\n",
    "seq_gp = df.groupby('sequence_id')\n",
    "X_list, y_list, id_list = [], [], []\n",
    "for seq_id, seq in seq_gp:\n",
    "    mat = preprocess_sequence(seq, feature_cols, scaler)\n",
    "    X_list.append(mat)\n",
    "    y_list.append(seq['gesture_int'].iloc[0])\n",
    "    id_list.append(seq_id)\n",
    "    # lens.append(len(mat))\n",
    "\n",
    "pad_len = PAD_PERCENTILE#int(np.percentile(lens, PAD_PERCENTILE))\n",
    "print(pad_len)\n",
    "np.save(EXPORT_DIR / \"sequence_maxlen.npy\", pad_len)\n",
    "np.save(EXPORT_DIR / \"feature_cols.npy\", np.array(feature_cols))\n",
    "id_list = np.array(id_list)\n",
    "X_list_all = pad_sequences_torch(X_list, maxlen=pad_len, padding='pre', truncating='pre')\n",
    "y_list_all = np.eye(len(le.classes_))[y_list].astype(np.float32)  # One-hot encoding\n",
    "\n",
    "augmenter = Augment(\n",
    "    p_jitter=0.9844818619033621, sigma=0.03291295776089293, scale_range=(0.7542342630597011,1.1625052821731077),\n",
    "    p_dropout=0.41782786013520684,\n",
    "    p_moda=0.3910622476959722, drift_std=0.0040285239353308015, drift_max=0.3929358950258158    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4301869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=random_state)\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c42a91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loader ....\n",
      "Model Creation ....\n",
      "Number of batches in train_loader: 101\n",
      "▶ Starting training...\n",
      "X shape torch.Size([64, 100, 332]) | y shape torch.Size([64])\n",
      "Epoch 0 | Train Loss: 2.9865 | Train Acc: 0.0599\n",
      "Epoch 0 | Val Acc: 0.0787\n",
      "Epoch 1 | Train Loss: 2.7727 | Train Acc: 0.1377\n",
      "Epoch 1 | Val Acc: 0.1601\n",
      "Epoch 2 | Train Loss: 2.4856 | Train Acc: 0.2226\n",
      "Epoch 2 | Val Acc: 0.2405\n",
      "Epoch 3 | Train Loss: 2.2341 | Train Acc: 0.2876\n",
      "Epoch 3 | Val Acc: 0.2986\n",
      "Epoch 4 | Train Loss: 2.0387 | Train Acc: 0.3277\n",
      "Epoch 4 | Val Acc: 0.3388\n",
      "Epoch 5 | Train Loss: 1.8661 | Train Acc: 0.3796\n",
      "Epoch 5 | Val Acc: 0.3911\n",
      "Epoch 6 | Train Loss: 1.7388 | Train Acc: 0.4115\n",
      "Epoch 6 | Val Acc: 0.4220\n",
      "Epoch 7 | Train Loss: 1.6313 | Train Acc: 0.4356\n",
      "Epoch 7 | Val Acc: 0.4483\n",
      "Epoch 8 | Train Loss: 1.5488 | Train Acc: 0.4692\n",
      "Epoch 8 | Val Acc: 0.4782\n",
      "Epoch 9 | Train Loss: 1.4740 | Train Acc: 0.4861\n",
      "Epoch 9 | Val Acc: 0.4943\n",
      "Epoch 10 | Train Loss: 1.4302 | Train Acc: 0.5009\n",
      "Epoch 10 | Val Acc: 0.5068\n",
      "Epoch 11 | Train Loss: 1.3966 | Train Acc: 0.5130\n",
      "Epoch 11 | Val Acc: 0.5139\n",
      "Epoch 12 | Train Loss: 1.3578 | Train Acc: 0.5257\n",
      "Epoch 12 | Val Acc: 0.5342\n",
      "Epoch 13 | Train Loss: 1.3144 | Train Acc: 0.5365\n",
      "Epoch 13 | Val Acc: 0.5384\n",
      "Epoch 14 | Train Loss: 1.2828 | Train Acc: 0.5405\n",
      "Epoch 14 | Val Acc: 0.5477\n",
      "Epoch 15 | Train Loss: 1.2568 | Train Acc: 0.5546\n",
      "Epoch 15 | Val Acc: 0.5601\n",
      "Epoch 16 | Train Loss: 1.2391 | Train Acc: 0.5574\n",
      "Epoch 16 | Val Acc: 0.5621\n",
      "Epoch 17 | Train Loss: 1.2230 | Train Acc: 0.5705\n",
      "Epoch 17 | Val Acc: 0.5732\n",
      "Epoch 18 | Train Loss: 1.2152 | Train Acc: 0.5692\n",
      "Epoch 18 | Val Acc: 0.5716\n",
      "Epoch 19 | Train Loss: 1.2047 | Train Acc: 0.5727\n",
      "Epoch 19 | Val Acc: 0.5794\n",
      "Epoch 20 | Train Loss: 1.1913 | Train Acc: 0.5739\n",
      "Epoch 20 | Val Acc: 0.5815\n",
      "Epoch 21 | Train Loss: 1.1797 | Train Acc: 0.5806\n",
      "Epoch 21 | Val Acc: 0.5844\n",
      "Epoch 22 | Train Loss: 1.1454 | Train Acc: 0.5975\n",
      "Epoch 22 | Val Acc: 0.5973\n",
      "Epoch 23 | Train Loss: 1.1542 | Train Acc: 0.5907\n",
      "Epoch 23 | Val Acc: 0.5964\n",
      "Epoch 24 | Train Loss: 1.1348 | Train Acc: 0.5942\n",
      "Epoch 24 | Val Acc: 0.6016\n",
      "Epoch 25 | Train Loss: 1.1101 | Train Acc: 0.6100\n",
      "Epoch 25 | Val Acc: 0.6132\n",
      "Epoch 26 | Train Loss: 1.1024 | Train Acc: 0.6091\n",
      "Epoch 26 | Val Acc: 0.6105\n",
      "Epoch 27 | Train Loss: 1.1049 | Train Acc: 0.6117\n",
      "Epoch 27 | Val Acc: 0.6112\n",
      "Epoch 28 | Train Loss: 1.1304 | Train Acc: 0.6032\n",
      "Epoch 28 | Val Acc: 0.6038\n",
      "Epoch 29 | Train Loss: 1.0915 | Train Acc: 0.6183\n",
      "Epoch 29 | Val Acc: 0.6158\n",
      "Epoch 30 | Train Loss: 1.0856 | Train Acc: 0.6106\n",
      "Epoch 30 | Val Acc: 0.6138\n",
      "Epoch 31 | Train Loss: 1.0624 | Train Acc: 0.6262\n",
      "Epoch 31 | Val Acc: 0.6283\n",
      "Epoch 32 | Train Loss: 1.0551 | Train Acc: 0.6204\n",
      "Epoch 32 | Val Acc: 0.6226\n",
      "Epoch 33 | Train Loss: 1.0628 | Train Acc: 0.6258\n",
      "Epoch 33 | Val Acc: 0.6243\n",
      "Epoch 34 | Train Loss: 1.0641 | Train Acc: 0.6230\n",
      "Epoch 34 | Val Acc: 0.6282\n",
      "Epoch 35 | Train Loss: 1.0484 | Train Acc: 0.6284\n",
      "Epoch 35 | Val Acc: 0.6282\n",
      "Epoch 36 | Train Loss: 1.0395 | Train Acc: 0.6309\n",
      "Epoch 36 | Val Acc: 0.6331\n",
      "Epoch 37 | Train Loss: 1.0335 | Train Acc: 0.6317\n",
      "Epoch 37 | Val Acc: 0.6321\n",
      "Epoch 38 | Train Loss: 1.0553 | Train Acc: 0.6261\n",
      "Epoch 38 | Val Acc: 0.6296\n",
      "Epoch 39 | Train Loss: 1.0121 | Train Acc: 0.6405\n",
      "Epoch 39 | Val Acc: 0.6355\n",
      "Epoch 40 | Train Loss: 1.0187 | Train Acc: 0.6357\n",
      "Epoch 40 | Val Acc: 0.6353\n",
      "Epoch 41 | Train Loss: 0.9797 | Train Acc: 0.6504\n",
      "Epoch 41 | Val Acc: 0.6502\n",
      "Epoch 42 | Train Loss: 0.9829 | Train Acc: 0.6457\n",
      "Epoch 42 | Val Acc: 0.6438\n",
      "Epoch 43 | Train Loss: 0.9873 | Train Acc: 0.6485\n",
      "Epoch 43 | Val Acc: 0.6507\n",
      "Epoch 44 | Train Loss: 0.9689 | Train Acc: 0.6513\n",
      "Epoch 44 | Val Acc: 0.6535\n",
      "Epoch 45 | Train Loss: 0.9794 | Train Acc: 0.6511\n",
      "Epoch 45 | Val Acc: 0.6478\n",
      "Epoch 46 | Train Loss: 0.9720 | Train Acc: 0.6564\n",
      "Epoch 46 | Val Acc: 0.6559\n",
      "Epoch 47 | Train Loss: 0.9609 | Train Acc: 0.6590\n",
      "Epoch 47 | Val Acc: 0.6610\n",
      "Epoch 48 | Train Loss: 0.9547 | Train Acc: 0.6627\n",
      "Epoch 48 | Val Acc: 0.6664\n",
      "Epoch 49 | Train Loss: 0.9549 | Train Acc: 0.6576\n",
      "Epoch 49 | Val Acc: 0.6556\n",
      "Epoch 50 | Train Loss: 0.9453 | Train Acc: 0.6595\n",
      "Epoch 50 | Val Acc: 0.6631\n",
      "Epoch 51 | Train Loss: 0.9297 | Train Acc: 0.6744\n",
      "Epoch 51 | Val Acc: 0.6713\n",
      "Epoch 52 | Train Loss: 0.9506 | Train Acc: 0.6654\n",
      "Epoch 52 | Val Acc: 0.6653\n",
      "Epoch 53 | Train Loss: 0.9166 | Train Acc: 0.6745\n",
      "Epoch 53 | Val Acc: 0.6749\n",
      "Epoch 54 | Train Loss: 0.9088 | Train Acc: 0.6750\n",
      "Epoch 54 | Val Acc: 0.6715\n",
      "Epoch 55 | Train Loss: 0.9242 | Train Acc: 0.6692\n",
      "Epoch 55 | Val Acc: 0.6698\n",
      "Epoch 56 | Train Loss: 0.9050 | Train Acc: 0.6740\n",
      "Epoch 56 | Val Acc: 0.6750\n",
      "Epoch 57 | Train Loss: 0.8904 | Train Acc: 0.6852\n",
      "Epoch 57 | Val Acc: 0.6832\n",
      "Epoch 58 | Train Loss: 0.9131 | Train Acc: 0.6731\n",
      "Epoch 58 | Val Acc: 0.6745\n",
      "Epoch 59 | Train Loss: 0.8833 | Train Acc: 0.6843\n",
      "Epoch 59 | Val Acc: 0.6842\n",
      "Epoch 60 | Train Loss: 0.8536 | Train Acc: 0.6960\n",
      "Epoch 60 | Val Acc: 0.6923\n",
      "Epoch 61 | Train Loss: 0.8688 | Train Acc: 0.6940\n",
      "Epoch 61 | Val Acc: 0.6928\n",
      "Epoch 62 | Train Loss: 0.8700 | Train Acc: 0.6866\n",
      "Epoch 62 | Val Acc: 0.6856\n",
      "Epoch 63 | Train Loss: 0.8213 | Train Acc: 0.7107\n",
      "Epoch 63 | Val Acc: 0.7060\n",
      "Epoch 64 | Train Loss: 0.8367 | Train Acc: 0.7053\n",
      "Epoch 64 | Val Acc: 0.6962\n",
      "Epoch 65 | Train Loss: 0.8422 | Train Acc: 0.6997\n",
      "Epoch 65 | Val Acc: 0.6956\n",
      "Epoch 66 | Train Loss: 0.8290 | Train Acc: 0.6999\n",
      "Epoch 66 | Val Acc: 0.6925\n",
      "Epoch 67 | Train Loss: 0.8303 | Train Acc: 0.7075\n",
      "Epoch 67 | Val Acc: 0.7016\n",
      "Epoch 68 | Train Loss: 0.8114 | Train Acc: 0.7112\n",
      "Epoch 68 | Val Acc: 0.7044\n",
      "Epoch 69 | Train Loss: 0.7985 | Train Acc: 0.7160\n",
      "Epoch 69 | Val Acc: 0.7103\n",
      "Epoch 70 | Train Loss: 0.7912 | Train Acc: 0.7263\n",
      "Epoch 70 | Val Acc: 0.7156\n",
      "Epoch 71 | Train Loss: 0.8072 | Train Acc: 0.7158\n",
      "Epoch 71 | Val Acc: 0.7094\n",
      "Epoch 72 | Train Loss: 0.7853 | Train Acc: 0.7290\n",
      "Epoch 72 | Val Acc: 0.7230\n",
      "Epoch 73 | Train Loss: 0.7633 | Train Acc: 0.7313\n",
      "Epoch 73 | Val Acc: 0.7246\n",
      "Epoch 74 | Train Loss: 0.7580 | Train Acc: 0.7310\n",
      "Epoch 74 | Val Acc: 0.7241\n",
      "Epoch 75 | Train Loss: 0.7401 | Train Acc: 0.7373\n",
      "Epoch 75 | Val Acc: 0.7300\n",
      "Epoch 76 | Train Loss: 0.7286 | Train Acc: 0.7392\n",
      "Epoch 76 | Val Acc: 0.7293\n",
      "Epoch 77 | Train Loss: 0.7324 | Train Acc: 0.7399\n",
      "Epoch 77 | Val Acc: 0.7303\n",
      "Epoch 78 | Train Loss: 0.7322 | Train Acc: 0.7449\n",
      "Epoch 78 | Val Acc: 0.7355\n",
      "Epoch 79 | Train Loss: 0.7117 | Train Acc: 0.7492\n",
      "Epoch 79 | Val Acc: 0.7385\n",
      "Epoch 80 | Train Loss: 0.7092 | Train Acc: 0.7491\n",
      "Epoch 80 | Val Acc: 0.7400\n",
      "Epoch 81 | Train Loss: 0.7101 | Train Acc: 0.7557\n",
      "Epoch 81 | Val Acc: 0.7467\n",
      "Epoch 82 | Train Loss: 0.6960 | Train Acc: 0.7559\n",
      "Epoch 82 | Val Acc: 0.7485\n",
      "Epoch 83 | Train Loss: 0.6952 | Train Acc: 0.7567\n",
      "Epoch 83 | Val Acc: 0.7480\n",
      "Epoch 84 | Train Loss: 0.6796 | Train Acc: 0.7618\n",
      "Epoch 84 | Val Acc: 0.7501\n",
      "Epoch 85 | Train Loss: 0.6517 | Train Acc: 0.7678\n",
      "Epoch 85 | Val Acc: 0.7567\n",
      "Epoch 86 | Train Loss: 0.6544 | Train Acc: 0.7746\n",
      "Epoch 86 | Val Acc: 0.7608\n",
      "Epoch 87 | Train Loss: 0.6472 | Train Acc: 0.7687\n",
      "Epoch 87 | Val Acc: 0.7535\n",
      "Epoch 88 | Train Loss: 0.6474 | Train Acc: 0.7712\n",
      "Epoch 88 | Val Acc: 0.7569\n",
      "Epoch 89 | Train Loss: 0.6320 | Train Acc: 0.7777\n",
      "Epoch 89 | Val Acc: 0.7654\n",
      "Epoch 90 | Train Loss: 0.6151 | Train Acc: 0.7803\n",
      "Epoch 90 | Val Acc: 0.7659\n",
      "Epoch 91 | Train Loss: 0.6160 | Train Acc: 0.7809\n",
      "Epoch 91 | Val Acc: 0.7687\n",
      "Epoch 92 | Train Loss: 0.6136 | Train Acc: 0.7842\n",
      "Epoch 92 | Val Acc: 0.7707\n",
      "Epoch 93 | Train Loss: 0.5832 | Train Acc: 0.7946\n",
      "Epoch 93 | Val Acc: 0.7790\n",
      "Epoch 94 | Train Loss: 0.6061 | Train Acc: 0.7806\n",
      "Epoch 94 | Val Acc: 0.7649\n",
      "Epoch 95 | Train Loss: 0.5820 | Train Acc: 0.8001\n",
      "Epoch 95 | Val Acc: 0.7835\n",
      "Epoch 96 | Train Loss: 0.5667 | Train Acc: 0.8000\n",
      "Epoch 96 | Val Acc: 0.7820\n",
      "Epoch 97 | Train Loss: 0.5623 | Train Acc: 0.8045\n",
      "Epoch 97 | Val Acc: 0.7852\n",
      "Epoch 98 | Train Loss: 0.5582 | Train Acc: 0.8031\n",
      "Epoch 98 | Val Acc: 0.7882\n",
      "Epoch 99 | Train Loss: 0.5606 | Train Acc: 0.8060\n",
      "Epoch 99 | Val Acc: 0.7871\n",
      "Epoch 100 | Train Loss: 0.5414 | Train Acc: 0.8123\n",
      "Epoch 100 | Val Acc: 0.7944\n",
      "Epoch 101 | Train Loss: 0.5400 | Train Acc: 0.8139\n",
      "Epoch 101 | Val Acc: 0.7968\n",
      "Epoch 102 | Train Loss: 0.5318 | Train Acc: 0.8113\n",
      "Epoch 102 | Val Acc: 0.7917\n",
      "Epoch 103 | Train Loss: 0.5182 | Train Acc: 0.8238\n",
      "Epoch 103 | Val Acc: 0.8022\n",
      "Epoch 104 | Train Loss: 0.5250 | Train Acc: 0.8232\n",
      "Epoch 104 | Val Acc: 0.8036\n",
      "Epoch 105 | Train Loss: 0.5100 | Train Acc: 0.8260\n",
      "Epoch 105 | Val Acc: 0.8038\n",
      "Epoch 106 | Train Loss: 0.5115 | Train Acc: 0.8241\n",
      "Epoch 106 | Val Acc: 0.8027\n",
      "Epoch 107 | Train Loss: 0.5047 | Train Acc: 0.8280\n",
      "Epoch 107 | Val Acc: 0.8041\n",
      "Epoch 108 | Train Loss: 0.5071 | Train Acc: 0.8292\n",
      "Epoch 108 | Val Acc: 0.8084\n",
      "Epoch 109 | Train Loss: 0.5041 | Train Acc: 0.8255\n",
      "Epoch 109 | Val Acc: 0.8044\n",
      "Epoch 110 | Train Loss: 0.4720 | Train Acc: 0.8393\n",
      "Epoch 110 | Val Acc: 0.8147\n",
      "Epoch 111 | Train Loss: 0.4746 | Train Acc: 0.8439\n",
      "Epoch 111 | Val Acc: 0.8182\n",
      "Epoch 112 | Train Loss: 0.4899 | Train Acc: 0.8342\n",
      "Epoch 112 | Val Acc: 0.8109\n",
      "Epoch 113 | Train Loss: 0.4793 | Train Acc: 0.8410\n",
      "Epoch 113 | Val Acc: 0.8165\n",
      "Epoch 114 | Train Loss: 0.4854 | Train Acc: 0.8331\n",
      "Epoch 114 | Val Acc: 0.8104\n",
      "Epoch 115 | Train Loss: 0.4862 | Train Acc: 0.8369\n",
      "Epoch 115 | Val Acc: 0.8152\n",
      "Epoch 116 | Train Loss: 0.4715 | Train Acc: 0.8434\n",
      "Epoch 116 | Val Acc: 0.8181\n",
      "Epoch 117 | Train Loss: 0.4804 | Train Acc: 0.8407\n",
      "Epoch 117 | Val Acc: 0.8173\n",
      "Epoch 118 | Train Loss: 0.4783 | Train Acc: 0.8374\n",
      "Epoch 118 | Val Acc: 0.8142\n",
      "Epoch 119 | Train Loss: 0.4866 | Train Acc: 0.8306\n",
      "Epoch 119 | Val Acc: 0.8099\n",
      "Epoch 120 | Train Loss: 0.4792 | Train Acc: 0.8377\n",
      "Epoch 120 | Val Acc: 0.8135\n",
      "Epoch 121 | Train Loss: 0.4740 | Train Acc: 0.8383\n",
      "Epoch 121 | Val Acc: 0.8152\n",
      "Epoch 122 | Train Loss: 0.4700 | Train Acc: 0.8386\n",
      "Epoch 122 | Val Acc: 0.8147\n",
      "Epoch 123 | Train Loss: 0.4857 | Train Acc: 0.8369\n",
      "Epoch 123 | Val Acc: 0.8130\n",
      "Epoch 124 | Train Loss: 0.4984 | Train Acc: 0.8266\n",
      "Epoch 124 | Val Acc: 0.8046\n",
      "fold: 0 val_all_acc: 0.8250\n",
      "✔ Training done – artefacts saved in C:\\Users\\konno\\SynologyDrive\\datasciense\\projects_foler\\1_kaggle\\CMI\\cmi-detect-behavior-with-sensor-data\\CMI3_pytorch\n",
      "Data Loader ....\n",
      "Model Creation ....\n",
      "Number of batches in train_loader: 101\n",
      "▶ Starting training...\n",
      "X shape torch.Size([64, 100, 332]) | y shape torch.Size([64])\n",
      "Epoch 0 | Train Loss: 2.9176 | Train Acc: 0.0705\n",
      "Epoch 0 | Val Acc: 0.0897\n",
      "Epoch 1 | Train Loss: 2.7234 | Train Acc: 0.1453\n",
      "Epoch 1 | Val Acc: 0.1653\n",
      "Epoch 2 | Train Loss: 2.4476 | Train Acc: 0.2158\n",
      "Epoch 2 | Val Acc: 0.2302\n",
      "Epoch 3 | Train Loss: 2.2178 | Train Acc: 0.2809\n",
      "Epoch 3 | Val Acc: 0.2955\n",
      "Epoch 4 | Train Loss: 2.0250 | Train Acc: 0.3270\n",
      "Epoch 4 | Val Acc: 0.3435\n",
      "Epoch 5 | Train Loss: 1.8710 | Train Acc: 0.3639\n",
      "Epoch 5 | Val Acc: 0.3757\n",
      "Epoch 6 | Train Loss: 1.7313 | Train Acc: 0.4152\n",
      "Epoch 6 | Val Acc: 0.4250\n",
      "Epoch 7 | Train Loss: 1.6352 | Train Acc: 0.4403\n",
      "Epoch 7 | Val Acc: 0.4479\n",
      "Epoch 8 | Train Loss: 1.5529 | Train Acc: 0.4664\n",
      "Epoch 8 | Val Acc: 0.4712\n",
      "Epoch 9 | Train Loss: 1.4720 | Train Acc: 0.4862\n",
      "Epoch 9 | Val Acc: 0.4924\n",
      "Epoch 10 | Train Loss: 1.4223 | Train Acc: 0.5046\n",
      "Epoch 10 | Val Acc: 0.5097\n",
      "Epoch 11 | Train Loss: 1.3835 | Train Acc: 0.5073\n",
      "Epoch 11 | Val Acc: 0.5072\n",
      "Epoch 12 | Train Loss: 1.3484 | Train Acc: 0.5159\n",
      "Epoch 12 | Val Acc: 0.5263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     53\u001b[39m total = \u001b[32m0\u001b[39m \u001b[38;5;66;03m# --- initialization\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# print(\"debug 101\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(\"debug 201\")\u001b[39;49;00m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\kaggle1\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\kaggle1\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\kaggle1\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\kaggle1\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\kaggle1\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         collate(samples, collate_fn_map=collate_fn_map)\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(skf.split(id_list, np.argmax(y_list_all, axis=1))):\n",
    "\n",
    "    train_list= X_list_all[train_idx]\n",
    "    train_y_list= y_list_all[train_idx]\n",
    "    val_list = X_list_all[val_idx]\n",
    "    val_y_list= y_list_all[val_idx]\n",
    "\n",
    "    print(\"Data Loader ....\")\n",
    "    # Data loaders\n",
    "    train_dataset = CMI3Dataset(train_list, train_y_list, maxlen, mode=\"train\", imu_dim=len(imu_cols),\n",
    "                            augment=augmenter)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0,drop_last=True)\n",
    "\n",
    "    val_dataset = CMI3Dataset(val_list, val_y_list, maxlen, mode=\"val\")\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,drop_last=True)\n",
    "\n",
    "    print(\"Model Creation ....\")\n",
    "    # Model\n",
    "    model = TwoBranchModel(maxlen, len(imu_cols), len(tof_cols), \n",
    "                    len(le.classes_)).to(device)\n",
    "    ema = EMA(model, decay=0.999)\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = Adam(model.parameters(), lr=LR_INIT, weight_decay=WD)\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5*steps_per_epoch)\n",
    "    \n",
    "    steps_per_epoch = len(train_loader)\n",
    "    nbatch = len(train_loader)\n",
    "    warmup = epochs_warmup * nbatch\n",
    "    nsteps = EPOCHS * nbatch\n",
    "    scheduler = CosineLRScheduler(optimizer,\n",
    "                        warmup_t=warmup, warmup_lr_init=warmup_lr_init, warmup_prefix=True,\n",
    "                        t_initial=(nsteps - warmup), lr_min=lr_min) \n",
    "\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_best_acc = 0.0\n",
    "    i_scheduler = 0\n",
    "    \n",
    "    print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "\n",
    "    # Training loop\n",
    "    print(\"▶ Starting training...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "        correct = 0  # --- initialization\n",
    "        total = 0 # --- initialization\n",
    "        # print(\"debug 101\")\n",
    "        for X, y in (train_loader):  \n",
    "            # print(\"debug 201\")\n",
    "            X, y = X.float().to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X)\n",
    "\n",
    "            loss = -torch.sum(F.log_softmax(logits, dim=1) * y, dim=1).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ema.update(model)\n",
    "            train_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            train_targets.extend(y.argmax(dim=1).cpu().numpy())\n",
    "            scheduler.step(i_scheduler)\n",
    "            i_scheduler +=1\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            pred_labels = logits.argmax(dim=1)\n",
    "            y = y.argmax(dim=1)\n",
    "            correct += (pred_labels == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            print(f\"X shape {X.shape} | y shape {y.shape}\")\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss / len(train_loader):.4f} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            for X, y in (val_loader):  \n",
    "                half = BATCH_SIZE // 2         \n",
    "\n",
    "                x_front = X[:half]               \n",
    "                x_back  = X[half:].clone()      \n",
    "                \n",
    "                x_back[:, :, 7:] = 0.0    \n",
    "                X = torch.cat([x_front, x_back], dim=0)  # (B, C, T)\n",
    "                X, y = X.float().to(device), y.to(device)\n",
    "                \n",
    "                logits = model(X)\n",
    "                val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "                val_targets.extend(y.argmax(dim=1).cpu().numpy())\n",
    "                \n",
    "                loss = F.cross_entropy(logits, y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                pred_labels = logits.argmax(1)\n",
    "                true_labels = y.argmax(1)\n",
    "                correct += (pred_labels == true_labels).sum().item()\n",
    "                total += y.size(0)\n",
    "            val_acc = correct / total\n",
    "            print(f\"Epoch {epoch} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        train_acc = CompetitionMetric().calculate_hierarchical_f1(\n",
    "            pd.DataFrame({'gesture': le.classes_[train_targets]}),\n",
    "            pd.DataFrame({'gesture': le.classes_[train_preds]}))\n",
    "        val_acc = CompetitionMetric().calculate_hierarchical_f1(\n",
    "            pd.DataFrame({'gesture': le.classes_[val_targets]}),\n",
    "            pd.DataFrame({'gesture': le.classes_[val_preds]}))\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "    models.append(model)\n",
    "    # Save model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'imu_dim': len(imu_cols),\n",
    "        'tof_dim': len(tof_cols),\n",
    "        'n_classes': len(le.classes_),\n",
    "        'pad_len': pad_len\n",
    "    }, EXPORT_DIR / f\"gesture_two_branch_fold{fold}.pth\")\n",
    "    print(f\"fold: {fold} val_all_acc: {val_acc:.4f}\")\n",
    "    print(\"✔ Training done – artefacts saved in\", EXPORT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd17710",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"▶ INFERENCE MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "imu_cols = [c for c in feature_cols if not (c.startswith('thm_') or c.startswith('tof_'))]\n",
    "tof_cols = [c for c in feature_cols if c.startswith('thm_') or c.startswith('tof_')]\n",
    "\n",
    "\n",
    "# Load model\n",
    "MODELS = [f'gesture_two_branch_fold{i}.pth' for i in range(5)]\n",
    "\n",
    "models = []\n",
    "for path in MODELS:\n",
    "    checkpoint = torch.load(PRETRAINED_DIR / path, map_location=device)\n",
    "    \n",
    "    model = TwoBranchModel(\n",
    "        checkpoint['pad_len'], \n",
    "        checkpoint['imu_dim'], \n",
    "        checkpoint['tof_dim'], \n",
    "        checkpoint['n_classes']\n",
    "        ).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "print(\"  model, scaler, pads loaded – ready for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure gesture_classes exists in both modes\n",
    "if TRAIN:\n",
    "    gesture_classes = le.classes_\n",
    "\n",
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    \"\"\"Prediction function for Kaggle competition\"\"\"\n",
    "    global gesture_classes\n",
    "    if gesture_classes is None:\n",
    "        gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "    df_seq = sequence.to_pandas()\n",
    "    mat = preprocess_sequence(df_seq, feature_cols, scaler)\n",
    "    pad = pad_sequences_torch([mat], maxlen=pad_len, padding='pre', truncating='pre')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x = torch.FloatTensor(pad).to(device)\n",
    "        outputs = None\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            p = torch.softmax(model(x), dim=1)\n",
    "            if outputs is None: outputs = p\n",
    "            else: outputs += p\n",
    "        outputs /= len(models)\n",
    "        \n",
    "        idx = int(outputs.argmax(dim=1)[0].cpu().numpy())\n",
    "    \n",
    "    return str(gesture_classes[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
