{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b79b6d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json, joblib\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82880947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ imports ready · torch 2.7.1+cu128 device : cuda\n"
     ]
    }
   ],
   "source": [
    "# (Competition metric will only be imported when TRAINing)\n",
    "TRAIN = True                     # ← set to True when you want to train\n",
    "\n",
    "class config:\n",
    "    AMP = False\n",
    "    BATCH_SIZE_TRAIN = 8 #32\n",
    "    BATCH_SIZE_VALID = 8 #32\n",
    "    DEBUG = False\n",
    "    EPOCHS = 2  #30\n",
    "    FOLDS = 5\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    LEARNING_RATE = 1e-3\n",
    "    MAX_GRAD_NORM = 1e7\n",
    "    NUM_WORKERS = 0 # multiprocessing.cpu_count()\n",
    "    PRINT_FREQ = 20\n",
    "    SEED = 20\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    PAD_PERCENTILE = 95\n",
    "\n",
    "class paths:\n",
    "    BASE_DIR = Path(\"C:/Users/konno/SynologyDrive/datasciense/projects_foler/1_kaggle/CMI/cmi-detect-behavior-with-sensor-data\")\n",
    "    \n",
    "    OUTPUT_DIR = BASE_DIR / \"output-02-wavenet\"\n",
    "    TEST_CSV = BASE_DIR / \"test.csv\"\n",
    "    TEST_DEMOGRAPHICS = BASE_DIR / \"test_demographics.csv\"\n",
    "    TRAIN_CSV = BASE_DIR / \"train.csv\"\n",
    "    TRAIN_DEMOGRAPHICS = BASE_DIR / \"train_demographics.csv\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "print(\"▶ imports ready · torch\", torch.__version__, \"device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "179bbdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7803102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list[str], scaler: StandardScaler):\n",
    "    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n",
    "    return scaler.transform(mat).astype('float32')\n",
    "\n",
    "# MixUp the data argumentation in order to regularize the neural network. \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, config, df: pd.DataFrame, X: np.ndarray, y: np.ndarray\n",
    "    ): \n",
    "        \n",
    "        self.config = config\n",
    "        self.df = df\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.indexes = self.df.sequence_id.unique()\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Length of dataset.\n",
    "        \"\"\"\n",
    "        return len(self.indexes)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get one item.\n",
    "        \"\"\"\n",
    "        # print(f\"[CustomDataset] Dataset size: {len(self.X)}, Index requested: {index}\")\n",
    "        \n",
    "        sequence_id = self.indexes[index]\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        output = {\n",
    "            \"X\": torch.tensor(X, dtype=torch.float32),\n",
    "            \"y\": torch.tensor(y, dtype=torch.long),\n",
    "            \"sequence_id\": sequence_id\n",
    "        }\n",
    "        return output     \n",
    "    \n",
    "class MixupDataset(CustomDataset):\n",
    "    def __init__(self, *args, alpha=0.4, mixup_prob=0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.mixup_prob = mixup_prob\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(f\"[CustomDataset] Dataset size: {len(self.X)}, Index requested: {index}\")\n",
    "        if np.random.rand() < self.mixup_prob:\n",
    "            # Mixup\n",
    "            idx1 = index\n",
    "            idx2 = np.random.randint(0, len(self))\n",
    "\n",
    "            X1 = self.X[idx1]\n",
    "            y1 = self.y[idx1]\n",
    "\n",
    "            X2 = self.X[idx2]\n",
    "            y2 = self.y[idx2]\n",
    "\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "\n",
    "            X_mix = lam * X1 + (1 - lam) * X2\n",
    "            y_mix = lam * y1 + (1 - lam) * y2\n",
    "\n",
    "            return {\n",
    "                \"X\": torch.tensor(X_mix, dtype=torch.float32),\n",
    "                \"y\": torch.tensor(y_mix, dtype=torch.long),\n",
    "                \"sequence_id\": self.indexes[idx1],  # you can decide what ID to keep\n",
    "            }\n",
    "\n",
    "        # No mixup\n",
    "        return super().__getitem__(index)\n",
    "    \n",
    "# train_dataset = MixupDataset(config, df_train, X_tr, y_tr, y_soft_tr)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE_TRAIN, shuffle=True)\n",
    "# val_dataset = CustomDataset(config, df_train, X_val, y_val, y_soft_val)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE_VALID, shuffle=True)\n",
    "\n",
    "def pad_or_truncate(seq, max_len, mode=TRAIN, pad_value=0.0, dtype=np.float32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pads or truncates a sequence to a fixed length.\n",
    "\n",
    "    Parameters:\n",
    "    - seq: np.ndarray of shape (L, D)\n",
    "    - max_len: int, desired sequence length\n",
    "    - mode: bool, True = random pad, False = regular pad\n",
    "    - pad_value: float or int, value to use for padding\n",
    "    - dtype: np.dtype, dtype for the output array\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray of shape (max_len, D)\n",
    "    \"\"\"\n",
    "    # print(\"sequence shape\", seq.shape)\n",
    "    L, D = seq.shape\n",
    "    # print(\"mode = \", mode)\n",
    "\n",
    "    if L > max_len:\n",
    "        return seq[:max_len] # truncate if too long\n",
    "\n",
    "    elif L < max_len:\n",
    "        total_padding = max_len - L\n",
    "        \n",
    "        if mode:\n",
    "            pad_start = np.random.randint(0, total_padding + 1)\n",
    "            pad_end = total_padding - pad_start\n",
    "            \n",
    "        else:\n",
    "            pad_start = 0\n",
    "            pad_end = total_padding\n",
    "\n",
    "        start_padding = np.full((pad_start, D), pad_value, dtype=dtype)\n",
    "        end_padding = np.full((pad_end, D), pad_value, dtype=dtype)\n",
    "        padded = np.vstack((start_padding, seq, end_padding))\n",
    "        # print(\"padded shape\", padded.shape)\n",
    "        return padded\n",
    "\n",
    "    else:\n",
    "        return seq.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0242b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    \n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "             \n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "\n",
    "            # Calculate the relative rotation\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            \n",
    "            # Convert delta rotation to angular velocity vector\n",
    "            # The rotation vector (Euler axis * angle) scaled by 1/dt\n",
    "            # is a good approximation for small delta_rot\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            # If quaternion is invalid, angular velocity remains zero\n",
    "            pass\n",
    "            \n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0 # Или np.nan, в зависимости от желаемого поведения\n",
    "            continue\n",
    "        try:\n",
    "            # Преобразование кватернионов в объекты Rotation\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "\n",
    "            # Вычисление углового расстояния: 2 * arccos(|real(p * q*)|)\n",
    "            # где p* - сопряженный кватернион q\n",
    "            # В scipy.spatial.transform.Rotation, r1.inv() * r2 дает относительное вращение.\n",
    "            # Угол этого относительного вращения - это и есть угловое расстояние.\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            \n",
    "            # Угол rotation vector соответствует угловому расстоянию\n",
    "            # Норма rotation vector - это угол в радианах\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 # В случае недействительных кватернионов\n",
    "            pass\n",
    "            \n",
    "    return angular_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f21133ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wave_Block(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, dilation_rates: int, kernel_size: int = 3):\n",
    "        \"\"\"\n",
    "        WaveNet building block.\n",
    "        :param in_channels: number of input channels.\n",
    "        :param out_channels: number of output channels.\n",
    "        :param dilation_rates: how many levels of dilations are used.\n",
    "        :param kernel_size: size of the convolving kernel.\n",
    "        \"\"\"\n",
    "        super(Wave_Block, self).__init__()\n",
    "        self.num_rates = dilation_rates\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "        # First conv: (B, in_channels, L) -> (B, out_channels, L)\n",
    "        self.convs.append(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=True)\n",
    "        )\n",
    "        \n",
    "        dilation_rates = [2 ** i for i in range(dilation_rates)]\n",
    "        for dilation_rate in dilation_rates:\n",
    "            # Filter conv: (B, out_channels, L) -> (B, out_channels, L)\n",
    "            self.filter_convs.append(\n",
    "                nn.Conv1d(\n",
    "                    out_channels, out_channels, kernel_size=kernel_size,\n",
    "                    padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate)\n",
    "            )\n",
    "            # Gate conv: (B, out_channels, L) -> (B, out_channels, L)\n",
    "            self.gate_convs.append(\n",
    "                nn.Conv1d(\n",
    "                    out_channels, out_channels, kernel_size=kernel_size,\n",
    "                    padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate)\n",
    "            )\n",
    "            # Residual conv: (B, out_channels, L) -> (B, out_channels, L)\n",
    "            self.convs.append(nn.Conv1d(out_channels, out_channels, kernel_size=1, bias=True))\n",
    "        \n",
    "        for i in range(len(self.convs)):\n",
    "            nn.init.xavier_uniform_(self.convs[i].weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.zeros_(self.convs[i].bias)\n",
    "\n",
    "        for i in range(len(self.filter_convs)):\n",
    "            nn.init.xavier_uniform_(self.filter_convs[i].weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.zeros_(self.filter_convs[i].bias)\n",
    "\n",
    "        for i in range(len(self.gate_convs)):\n",
    "            nn.init.xavier_uniform_(self.gate_convs[i].weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.zeros_(self.gate_convs[i].bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (B, in_channels, L)\n",
    "        x = self.convs[0](x)  # (B, in_channels, L) -> (B, out_channels, L)\n",
    "        res = x  # res: (B, out_channels, L)\n",
    "        for i in range(self.num_rates):\n",
    "            tanh_out = torch.tanh(self.filter_convs[i](x))  # (B, out_channels, L) -> (B, out_channels, L)\n",
    "            sigmoid_out = torch.sigmoid(self.gate_convs[i](x)) # (B, out_channels, L) -> (B, out_channels, L)\n",
    "            x = tanh_out * sigmoid_out  # (B, out_channels, L) * (B, out_channels, L) -> (B, out_channels, L)\n",
    "            x = self.convs[i + 1](x) # (B, out_channels, L) -> (B, out_channels, L)\n",
    "            res = res + x  # (B, out_channels, L) + (B, out_channels, L) -> (B, out_channels, L)\n",
    "        return res  # (B, out_channels, L)\n",
    "    \n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, input_channels: int = 1, kernel_size: int = 3):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                Wave_Block(input_channels, 32, 12, kernel_size),   # (B, input_channels, L) -> (B, 8, L)\n",
    "                Wave_Block(32, 64, 8, kernel_size),                # (B, 8, L) -> (B, 16, L)\n",
    "                Wave_Block(64, 128, 4, kernel_size),               # (B, 16, L) -> (B, 32, L)\n",
    "                Wave_Block(128, 256, 1, kernel_size),                # (B, 32, L) -> (B, 64, L)\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, L, input_channels) - typical input format\n",
    "        x = x.permute(0, 2, 1)  # (B, L, input_channels) -> (B, input_channels, L)\n",
    "        output = self.model(x)  # (B, input_channels, L) -> (B, 64, L)\n",
    "        return output  # (B, 64, L)\n",
    "\n",
    "\n",
    "class TemporalAttentionPooling(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim=64):\n",
    "        super(TemporalAttentionPooling, self).__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, hidden_dim, kernel_size=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(hidden_dim, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, C, L)\n",
    "        returns: (B, C)\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        attn_scores = self.attn(x)  # (B, 1, L)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # (B, 1, L)\n",
    "\n",
    "        # Weighted sum over time\n",
    "        weighted = x * attn_weights  # (B, C, L)\n",
    "        pooled = weighted.sum(dim=-1)  # (B, C)\n",
    "\n",
    "        return pooled\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.wavenet1 = WaveNet(input_channels=17)  # WaveNet: (B, input_channels, L) -> (B, 64, L)\n",
    "        self.wavenet2 = WaveNet(input_channels=25)  # WaveNet: (B, input_channels, L) -> (B, 64, L)\n",
    "        self.config = config\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool1d(1)  # (B, 64, L) -> (B, 64, 1)\n",
    "        self.dropout = 0.2\n",
    "        self.head_1 = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(512, 512*4), # (B, 512) -> (B, 512*4)  #add\n",
    "            ResidualBlock(512*4),  #add\n",
    "            nn.Linear(512*4, 512*4), # (B, 512*4) -> (B, 512*4)\n",
    "            nn.LayerNorm(512*4),  # (B, 512*4) -> (B, 512*4)  BatchNorm1d\n",
    "            nn.GELU(),  # (B, 512*4) -> (B, 512*4)  ReLu\n",
    "            nn.Dropout(self.dropout),  # (B, 512*4) -> (B, 512*4)\n",
    "            nn.Linear(512*4, num_classes)  # (B, 512*4) -> (B, num_classes)\n",
    "        )\n",
    "\n",
    "        # self.attn_pool = TemporalAttentionPooling(input_channels=256)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        # x: (B, L, input_channels) - typical input format\n",
    "        x1 = self.wavenet1(x[:, :, :17])  # (B, L, input_channels) -> (B, 256, L)\n",
    "        # print(\"shape of x1 :\", x1.shape)        \n",
    "        x2 = self.wavenet2(x[:, :, 17:])  # (B, L, input_channels) -> (B, 256, L)\n",
    "        # print(\"shape of x2 :\", x2.shape) \n",
    "        x1 = self.global_avg_pooling(x1)  # (B, 256, L) -> (B, 256, 1)\n",
    "        x2 = self.global_avg_pooling(x2)  # (B, 256, L) -> (B, 256, 1)\n",
    "        # x1 = self.attn_pool(x1)  # (B, 256, L) -> (B, 256, 1)\n",
    "        # x2 = self.attn_pool(x2)  # (B, 256, L) -> (B, 256, 1)\n",
    "        x = torch.concatenate([x1, x2], axis=1) # (B, 512, 1)\n",
    "        # print(\"shape of x :\", x.shape)\n",
    "        x = x.squeeze(-1) # (B, 512)\n",
    "        z1 = self.head_1(x)  # (B, 512) -> (B, num_classes)\n",
    "        return z1  # (B, num_classes)\n",
    "\n",
    "\n",
    "# model = CustomModel(num_classes=9)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22855db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomModel(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(CustomModel, self).__init__()\n",
    "#         self.wavenet1 = WaveNet(input_channels=3)  # WaveNet: (B, input_channels, L) -> (B, 64, L)\n",
    "#         self.wavenet2 = WaveNet(input_channels=4)  # WaveNet: (B, input_channels, L) -> (B, 64, L)\n",
    "#         self.config = config\n",
    "#         self.global_avg_pooling = nn.AdaptiveAvgPool1d(1)  # (B, 64, L) -> (B, 64, 1)\n",
    "#         self.dropout = 0.2\n",
    "#         self.head_1 = nn.Sequential(\n",
    "#             nn.Flatten(), \n",
    "#             nn.Linear(512, 256), # (B, 64) -> (B, 64)\n",
    "#             nn.BatchNorm1d(256),  # (B, 64) -> (B, 64)\n",
    "#             nn.ReLU(),  # (B, 64) -> (B, 64)\n",
    "#             nn.Dropout(self.dropout),  # (B, 64) -> (B, 64)\n",
    "#             nn.Linear(256, num_classes)  # (B, 64) -> (B, num_classes)\n",
    "#         )\n",
    "#         self.head_2 = nn.Sequential(\n",
    "#             nn.Flatten(), \n",
    "#             nn.Linear(512, 256), # (B, 64) -> (B, 64)\n",
    "#             nn.BatchNorm1d(256),  # (B, 64) -> (B, 64)\n",
    "#             nn.ReLU(),  # (B, 64) -> (B, 64)\n",
    "#             nn.Dropout(self.dropout),  # (B, 64) -> (B, 64)\n",
    "#             nn.Linear(256, 1)  # (B, 64) -> (B, num_classes)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         \"\"\"\n",
    "#         Forward pass.\n",
    "#         \"\"\"\n",
    "#         # x: (B, L, input_channels) - typical input format\n",
    "#         x1 = self.wavenet1(x[:, :, 0:3])  # (B, L, input_channels) -> (B, 64, L)\n",
    "#         x1 = self.global_avg_pooling(x1)  # (B, 64, L) -> (B, 64, 1)\n",
    "#         x2 = self.wavenet2(x[:, :, 3:])  # (B, L, input_channels) -> (B, 64, L)\n",
    "#         x2 = self.global_avg_pooling(x2)  # (B, 64, L) -> (B, 64, 1)\n",
    "#         y = torch.concatenate([x1, x2], axis=1) # (B, 128)\n",
    "#         z1 = self.head_1(y)  # (B, 64) -> (B, num_classes)\n",
    "#         z2 = self.head_2(y)  # (B, 64) -> (B, num_classes)\n",
    "#         return z1, z2  # (B, num_classes)\n",
    "\n",
    "# model = CustomModel(num_classes=9)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "# ######################################\n",
    "# ######## TWO WAVENETS ################\n",
    "# ######################################\n",
    "# class CustomModel(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(CustomModel, self).__init__()\n",
    "#         self.wavenet1 = WaveNet(input_channels=21)  # WaveNet: (B, input_channels, L) -> (B, 64, L)\n",
    "#         self.wavenet2 = WaveNet(input_channels=21)  # WaveNet: (B, input_channels, L) -> (B, 64, L)\n",
    "#         self.config = config\n",
    "#         self.global_avg_pooling = nn.AdaptiveAvgPool1d(1)  # (B, 64, L) -> (B, 64, 1)\n",
    "#         self.dropout = 0.2\n",
    "#         self.head_1 = nn.Sequential(\n",
    "#             nn.Flatten(), \n",
    "#             nn.Linear(128, 256), # (B, 64) -> (B, 64)\n",
    "#             nn.BatchNorm1d(256),  # (B, 64) -> (B, 64)\n",
    "#             nn.ReLU(),  # (B, 64) -> (B, 64)\n",
    "#             nn.Dropout(self.dropout),  # (B, 64) -> (B, 64)\n",
    "#             nn.Linear(256, num_classes)  # (B, 64) -> (B, num_classes)\n",
    "#         )\n",
    "#         self.head_2 = nn.Sequential(\n",
    "#             nn.Flatten(), \n",
    "#             nn.Linear(128, 256), # (B, 64) -> (B, 64)\n",
    "#             nn.BatchNorm1d(256),  # (B, 64) -> (B, 64)\n",
    "#             nn.ReLU(),  # (B, 64) -> (B, 64)\n",
    "#             nn.Dropout(self.dropout),  # (B, 64) -> (B, 64)\n",
    "#             nn.Linear(256, 1)  # (B, 64) -> (B, num_classes)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         \"\"\"\n",
    "#         Forward pass.\n",
    "#         \"\"\"\n",
    "#         # x: (B, L, input_channels) - typical input format\n",
    "#         x1 = self.wavenet1(x[:, :, :21])  # (B, L, input_channels) -> (B, 64, L)\n",
    "#         x2 = self.wavenet2(x[:, :, 21:])  # (B, L, input_channels) -> (B, 64, L)\n",
    "#         x1 = self.global_avg_pooling(x1)  # (B, 64, L) -> (B, 64, 1)\n",
    "#         x2 = self.global_avg_pooling(x2)  # (B, 64, L) -> (B, 64, 1)\n",
    "#         x = torch.concatenate([x1, x2], axis=1) # (B, 128)\n",
    "#         x = x.squeeze(-1) # (B, 64)\n",
    "#         z1 = self.head_1(x)  # (B, 64) -> (B, num_classes)\n",
    "#         z2 = self.head_2(x)  # (B, 64) -> (B, num_classes)\n",
    "#         return z1, z2  # (B, num_classes)\n",
    "# ######################################\n",
    "# ######## TWO WAVENETS ################\n",
    "# ######################################\n",
    "\n",
    "\n",
    "# ######################################\n",
    "# ######## THREE WAVENETS ##############\n",
    "# ######################################\n",
    "\n",
    "# class CustomModel(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(CustomModel, self).__init__()\n",
    "#         self.wavenet_imu  = WaveNet(input_channels=21)  # WaveNet: (B, input_channels, L) -> (B, 64, L)\n",
    "#         self.wavenet_thm  = WaveNet(input_channels=16)  # WaveNet: (B, input_channels, L) -> (B, 64, L)\n",
    "#         self.wavenet_tof  = WaveNet(input_channels=5)  # WaveNet: (B, input_channels, L) -> (B, 64, L)\n",
    "#         self.config = config\n",
    "#         self.global_avg_pooling = nn.AdaptiveAvgPool1d(1)  # (B, 64, L) -> (B, 64, 1)\n",
    "#         self.dropout = 0.2\n",
    "#         self.head_1 = nn.Sequential(\n",
    "#             nn.Flatten(), \n",
    "#             nn.Linear(192, 256), # (B, 64) -> (B, 64)\n",
    "#             nn.BatchNorm1d(256),  # (B, 64) -> (B, 64)\n",
    "#             nn.ReLU(),  # (B, 64) -> (B, 64)\n",
    "#             nn.Dropout(self.dropout),  # (B, 64) -> (B, 64)\n",
    "#             nn.Linear(256, num_classes)  # (B, 64) -> (B, num_classes)\n",
    "#         )\n",
    "#         self.head_2 = nn.Sequential(\n",
    "#             nn.Flatten(), \n",
    "#             nn.Linear(192, 256), # (B, 64) -> (B, 64)\n",
    "#             nn.BatchNorm1d(256),  # (B, 64) -> (B, 64)\n",
    "#             nn.ReLU(),  # (B, 64) -> (B, 64)\n",
    "#             nn.Dropout(self.dropout),  # (B, 64) -> (B, 64)\n",
    "#             nn.Linear(256, 1)  # (B, 64) -> (B, num_classes)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         \"\"\"\n",
    "#         Forward pass.\n",
    "#         \"\"\"\n",
    "#         # x: (B, L, input_channels) - typical input format\n",
    "#         x_imu = self.wavenet_imu(x[:, :, :21])\n",
    "#         x_thm = self.wavenet_thm(x[:, :, 21:37])\n",
    "#         x_tof = self.wavenet_tof(x[:, :, 37:])  # if ToF has 5 channels\n",
    "#         x_imu = self.global_avg_pooling(x_imu)  # (B, 64, L) -> (B, 64, 1)\n",
    "#         x_thm = self.global_avg_pooling(x_thm)  # (B, 64, L) -> (B, 64, 1)\n",
    "#         x_tof = self.global_avg_pooling(x_tof)  # (B, 64, L) -> (B, 64, 1)\n",
    "#         x = torch.cat([x_imu, x_thm, x_tof], dim=1)  #192\n",
    "#         x = x.squeeze(-1) # (B, 64)\n",
    "#         z1 = self.head_1(x)  # (B, 64) -> (B, num_classes)\n",
    "#         z2 = self.head_2(x)  # (B, 64) -> (B, num_classes)\n",
    "#         return z1, z2  # (B, num_classes)\n",
    "\n",
    "# ######################################\n",
    "# ######## THREE WAVENETS ##############\n",
    "# ######################################\n",
    "\n",
    "# # ######################################\n",
    "# # ######## SINGLE WAVENETS #############\n",
    "# # ######################################\n",
    "# class CustomModel(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(CustomModel, self).__init__()\n",
    "#         self.wavenet = WaveNet(input_channels=42)  # WaveNet: (B, input_channels, L) -> (B, 64, L)\n",
    "#         self.config = config\n",
    "#         self.global_avg_pooling = nn.AdaptiveAvgPool1d(1)  # (B, 64, L) -> (B, 64, 1)\n",
    "#         self.dropout = 0.2\n",
    "#         self.head_1 = nn.Sequential(\n",
    "#             nn.Flatten(), \n",
    "#             nn.Linear(256, 256*4), # (B, 256) -> (B, 256*4)\n",
    "#             nn.BatchNorm1d(256*4),  # (B, 256*4) -> (B, 256*4)\n",
    "#             nn.ReLU(),  # (B, 256*4) -> (B, 256*4)\n",
    "#             nn.Dropout(self.dropout),  # (B, 256*4) -> (B, 256*4)\n",
    "#             nn.Linear(256*4, num_classes)  # (B, 256*4) -> (B, num_classes)\n",
    "#         )\n",
    "#         # self.head_2 = nn.Sequential(\n",
    "#         #     nn.Flatten(), \n",
    "#         #     nn.Linear(64, 256), # (B, 64) -> (B, 64)\n",
    "#         #     nn.BatchNorm1d(256),  # (B, 64) -> (B, 64)\n",
    "#         #     nn.ReLU(),  # (B, 64) -> (B, 64)\n",
    "#         #     nn.Dropout(self.dropout),  # (B, 64) -> (B, 64)\n",
    "#         #     nn.Linear(256, 1)  # (B, 64) -> (B, num_classes)\n",
    "#         # )\n",
    "        \n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         \"\"\"\n",
    "#         Forward pass.\n",
    "#         \"\"\"\n",
    "#         # x: (B, L, input_channels) - typical input format\n",
    "#         # print(\"[CustomModel] - forward x shape 101\", x.shape)\n",
    "#         x = self.wavenet(x)  # (B, L, input_channels) -> (B, 64, L)\n",
    "#         # print(\"[CustomModel] - forward x shape 102\", x.shape)\n",
    "#         x = self.global_avg_pooling(x)  # (B, 64, L) -> (B, 64, 1)\n",
    "#         # print(\"[CustomModel] - forward x shape 103\", x.shape)\n",
    "#         x = x.squeeze(-1) # (B, 256)\n",
    "#         # print(\"[CustomModel] - forward x shape 104\", x.shape)\n",
    "#         z1 = self.head_1(x)  # (B, 64) -> (B, num_classes)\n",
    "#         # z2 = self.head_2(x)  # (B, 64) -> (B, num_classes)\n",
    "#         return z1  #, z2  # (B, num_classes)\n",
    "\n",
    "# ######################################\n",
    "# ######## SINGLE WAVENETS #############\n",
    "# ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b507c241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ TRAIN MODE – loading dataset …\n",
      "df for group shape : (574945, 348)\n",
      " 0/8 Calculating base engineered IMU features (magnitude, angle) ...\n",
      " 1/8 Calculating engineered IMU derivatives (jerk, angular velocity) for original acc_mag ...\n",
      " 2/8 Removing gravity and calculating linear acceleration features...\n",
      " 3/8 Calculating angular velocity from quaternion derivatives...\n",
      " 4/8 Calculating angular distance between successive quaternions...\n",
      " 5/8 Calculating imu_cols_base ...\n",
      " 6/8 Calculating tof_aggregated_cols_template...\n",
      " IMU (incl. engineered & derivatives) 17 | THM (5) + Aggregated TOF 25 | total 42 features\n",
      " 7/8 calculating tof tof_i_mean/std/min/max...\n",
      " 8/8 ToF completed and Fitting StandardScaler...\n",
      " Last/8 Scaling and padding sequences...\n"
     ]
    }
   ],
   "source": [
    "### DATA CREATION\n",
    "print(\"▶ TRAIN MODE – loading dataset …\")\n",
    "df = pd.read_csv(paths.TRAIN_CSV)\n",
    "\n",
    "train_dem_df = pd.read_csv(paths.TRAIN_DEMOGRAPHICS)\n",
    "df_for_groups = pd.merge(df.copy(), train_dem_df, on='subject', how='left')\n",
    "print(\"df for group shape :\", df_for_groups.shape)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "np.save(paths.OUTPUT_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "gesture_classes = le.classes_\n",
    "\n",
    "\n",
    "print(\" 0/8 Calculating base engineered IMU features (magnitude, angle) ...\")\n",
    "df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "df['rot_angle'] = 2* np.arccos(df['rot_w'].clip(-1, 1))\n",
    "\n",
    "\n",
    "print(\" 1/8 Calculating engineered IMU derivatives (jerk, angular velocity) for original acc_mag ...\")\n",
    "df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "\n",
    "\n",
    "print(\" 2/8 Removing gravity and calculating linear acceleration features...\")\n",
    "\n",
    "linear_accel_list = []\n",
    "for _, group in df.groupby('sequence_id'):\n",
    "    acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n",
    "    rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "    linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
    "    linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n",
    "\n",
    "df_linear_accel = pd.concat(linear_accel_list)\n",
    "df = pd.concat([df, df_linear_accel], axis=1)\n",
    "\n",
    "df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "\n",
    "\n",
    "print(\" 3/8 Calculating angular velocity from quaternion derivatives...\")\n",
    "angular_vel_list = []\n",
    "for _, group in df.groupby('sequence_id'):\n",
    "    rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "    angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
    "    angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n",
    "\n",
    "df_angular_vel = pd.concat(angular_vel_list)\n",
    "df = pd.concat([df, df_angular_vel], axis=1)\n",
    "\n",
    "\n",
    "print(\" 4/8 Calculating angular distance between successive quaternions...\")\n",
    "angular_distance_list = []\n",
    "for _, group in df.groupby('sequence_id'):\n",
    "    rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "    angular_dist_group = calculate_angular_distance(rot_data_group)\n",
    "    angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n",
    "\n",
    "df_angular_distance = pd.concat(angular_distance_list)\n",
    "df = pd.concat([df, df_angular_distance], axis=1)\n",
    "\n",
    "meta_cols = { } # This was an empty dict in your provided code, keeping it as is.\n",
    "\n",
    "print(\" 5/8 Calculating imu_cols_base ...\")\n",
    "\n",
    "imu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n",
    "imu_cols_base.extend([c for c in df.columns if c.startswith('rot_') and c not in ['rot_angle', 'rot_angle_vel']])\n",
    "\n",
    "imu_engineered_features = [\n",
    "    'acc_mag', 'rot_angle',\n",
    "    'acc_mag_jerk', 'rot_angle_vel',\n",
    "    'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "    'angular_vel_x', 'angular_vel_y', 'angular_vel_z', # Existing new features\n",
    "    'angular_distance' # Added new feature\n",
    "]\n",
    "imu_cols = imu_cols_base + imu_engineered_features\n",
    "imu_cols = list(dict.fromkeys(imu_cols)) # Для удаления дубликатов\n",
    "\n",
    "thm_cols_original = [c for c in df.columns if c.startswith('thm_')]\n",
    "\n",
    "print(\" 6/8 Calculating tof_aggregated_cols_template...\")\n",
    "\n",
    "## tof data\n",
    "tof_aggregated_cols_template = []\n",
    "for i in range(1, 6):\n",
    "    tof_aggregated_cols_template.extend([f'tof_{i}_mean', f'tof_{i}_std', f'tof_{i}_min', f'tof_{i}_max'])\n",
    "\n",
    "final_feature_cols = imu_cols + thm_cols_original + tof_aggregated_cols_template\n",
    "imu_dim_final = len(imu_cols)\n",
    "tof_thm_aggregated_dim_final = len(thm_cols_original) + len(tof_aggregated_cols_template)\n",
    "\n",
    "print(f\" IMU (incl. engineered & derivatives) {imu_dim_final} | THM ({len(thm_cols_original)}) + Aggregated TOF {tof_thm_aggregated_dim_final} | total {len(final_feature_cols)} features\")\n",
    "np.save(paths.OUTPUT_DIR / \"feature_cols.npy\", np.array(final_feature_cols))\n",
    "\n",
    "print(\" 7/8 calculating tof tof_i_mean/std/min/max...\")\n",
    "\n",
    "seq_gp = df.groupby('sequence_id') \n",
    "\n",
    "all_steps_for_scaler_list = []\n",
    "X_list_unscaled, y_list_int_for_stratify, lens = [], [], [] \n",
    "\n",
    "for seq_id, seq_df_orig in seq_gp:\n",
    "    seq_df = seq_df_orig.copy()\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        pixel_cols_tof = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "        tof_sensor_data = seq_df[pixel_cols_tof].replace(-1, np.nan)\n",
    "        seq_df[f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n",
    "        seq_df[f'tof_{i}_std']  = tof_sensor_data.std(axis=1)\n",
    "        seq_df[f'tof_{i}_min']  = tof_sensor_data.min(axis=1)\n",
    "        seq_df[f'tof_{i}_max']  = tof_sensor_data.max(axis=1)\n",
    "    \n",
    "    mat_unscaled = seq_df[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    \n",
    "    all_steps_for_scaler_list.append(mat_unscaled)\n",
    "    X_list_unscaled.append(mat_unscaled)\n",
    "    y_list_int_for_stratify.append(seq_df['gesture_int'].iloc[0])\n",
    "    lens.append(len(mat_unscaled))\n",
    "\n",
    "\n",
    "# fit scaler\n",
    "print(\" 8/8 ToF completed and Fitting StandardScaler...\")\n",
    "all_steps_concatenated = np.concatenate(all_steps_for_scaler_list, axis=0)\n",
    "scaler = StandardScaler().fit(all_steps_concatenated)\n",
    "joblib.dump(scaler, paths.OUTPUT_DIR / \"scaler.pkl\")\n",
    "del all_steps_for_scaler_list, all_steps_concatenated\n",
    "\n",
    "# scale individual sequences\n",
    "print(\" Last/8 Scaling and padding sequences...\")\n",
    "X_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\n",
    "del X_list_unscaled\n",
    "\n",
    "# calculate pad length\n",
    "pad_len = int(np.percentile(lens, config.PAD_PERCENTILE))\n",
    "np.save(paths.OUTPUT_DIR / \"sequence_maxlen.npy\", pad_len)\n",
    "\n",
    "# padding section\n",
    "X_padded_np = np.stack([pad_or_truncate(seq, pad_len) for seq in X_scaled_list])\n",
    "X = torch.tensor(X_padded_np, dtype=torch.float32)  # shape: (N, T, D)\n",
    "del X_scaled_list\n",
    "\n",
    "y_int_for_stratify = np.array(y_list_int_for_stratify)\n",
    "y = F.one_hot(torch.tensor(y_int_for_stratify), num_classes=len(le.classes_)).float()\n",
    "\n",
    "# Suppose df_seq has exactly one row per sequence (same order as X)\n",
    "df_seq = df_for_groups.drop_duplicates('sequence_id').reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91bcfc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([8151, 127, 42])\n",
      "y shape: torch.Size([8151, 18])\n",
      "df_seq shape: (8151, 348)\n",
      "  Splitting data and preparing for training...\n",
      "  ...Data Loader\n",
      "  ...Data Loader completed\n"
     ]
    }
   ],
   "source": [
    "# Check the shape \n",
    "print(\"X shape:\", X.shape)              # (N_sequences, T, D)\n",
    "print(\"y shape:\", y.shape)              # (N_sequences, num_classes)\n",
    "print(\"df_seq shape:\", df_seq.shape)    # (N_unique_sequences, n_meta_columns)\n",
    "\n",
    "print(\"  Splitting data and preparing for training...\")\n",
    "# X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=82, stratify=y_int_for_stratify)\n",
    "X_tr, X_val, y_tr, y_val, df_train, df_val = train_test_split(\n",
    "    X, y, df_seq, test_size=0.2, random_state=82, stratify=y_int_for_stratify\n",
    ")\n",
    "\n",
    "y_train_int = y_tr.argmax(dim=1).cpu().numpy()  # Convert one-hot back to class indices\n",
    "cw_vals = compute_class_weight('balanced', classes=np.arange(len(le.classes_)), y=y_train_int)  #y=y_int_for_stratify\n",
    "\n",
    "# Data Loader\n",
    "print(\"  ...Data Loader\")\n",
    "train_dataset = MixupDataset(config, df_train, X_tr, y_tr)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE_TRAIN, shuffle=True, drop_last=True)\n",
    "val_dataset = CustomDataset(config, df_val, X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE_VALID, shuffle=True, drop_last=True)\n",
    "print(\"  ...Data Loader completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "058af5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 12056594\n",
      "Total number of classes: 18\n"
     ]
    }
   ],
   "source": [
    "model = CustomModel(num_classes=len(le.classes_))\n",
    "model.to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Total number of classes: {len(le.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f95d3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape xb torch.Size([8, 127, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "815it [00:39, 20.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 2.5883\n",
      "Epoch 0 | Val Acc: 0.0788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape xb torch.Size([8, 127, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "815it [00:38, 21.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 2.2553\n",
      "Epoch 1 | Val Acc: 0.1312\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "weights_tensor = torch.tensor(cw_vals, dtype=torch.float32).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights_tensor, label_smoothing=0.1)\n",
    "\n",
    "steps = []\n",
    "lrs = []\n",
    "best_val_acc = 0\n",
    "patience, patience_counter = 10, 0\n",
    "EPOCHS = config.EPOCHS\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,\n",
    "    epochs=config.EPOCHS,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.0,\n",
    "    anneal_strategy=\"cos\",\n",
    "    final_div_factor=100,\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in tqdm.tqdm(enumerate(train_loader)):\n",
    "        xb, yb = batch[\"X\"].to(device), batch[\"y\"].to(device)\n",
    "        if batch_idx == 0:\n",
    "            print(\"shape xb\", xb.shape)\n",
    "        optimizer.zero_grad()        \n",
    "        preds_cls = model(xb)\n",
    "        yb_indices = yb.argmax(dim=1)\n",
    "        loss = loss_fn(preds_cls, yb_indices)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # optional\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        lrs.append(scheduler.get_last_lr()[0])\n",
    "        steps.append(epoch * config.BATCH_SIZE_TRAIN + batch_idx)\n",
    "\n",
    "    print(f\"Epoch {epoch} | Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            xb, yb = batch[\"X\"].to(device), batch[\"y\"].to(device)\n",
    "            preds_cls = model(xb)\n",
    "            pred_labels = preds_cls.argmax(1)\n",
    "            true_labels = yb.argmax(1) if yb.ndim > 1 else yb  #.argmax(1)  val_loader comes from a standard dataset with \"y\" as class index (long), you don’t need argmax.\n",
    "            correct += (pred_labels == true_labels).sum().item()\n",
    "            total += yb.size(0)\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), paths.OUTPUT_DIR / \"best_model.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d18b6",
   "metadata": {},
   "source": [
    "0it [00:00, ?it/s]\n",
    "shape xb torch.Size([8, 127, 42])\n",
    "815it [00:33, 24.28it/s]\n",
    "Epoch 0 | Train Loss: 2.7902\n",
    "Epoch 0 | Val Acc: 0.1749\n",
    "3it [00:00, 21.27it/s]\n",
    "shape xb torch.Size([8, 127, 42])\n",
    "815it [00:31, 25.57it/s]\n",
    "Epoch 1 | Train Loss: 2.2463\n",
    "Epoch 1 | Val Acc: 0.2506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81fca264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(paths.OUTPUT_DIR / \"best_model.pt\"))\n",
    "# model.eval()\n",
    "# preds_val = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for xb, _ in val_loader:\n",
    "#         xb = xb.to(device)\n",
    "#         logits = model(xb)\n",
    "#         preds_val.append(logits.argmax(1).cpu())\n",
    "\n",
    "# preds_val = torch.cat(preds_val).numpy()\n",
    "# true_val_int = y_val.argmax(1).numpy()\n",
    "\n",
    "# # Evaluation\n",
    "# from cmi_2025_metric_copy_for_import import CompetitionMetric\n",
    "\n",
    "# h_f1 = CompetitionMetric().calculate_hierarchical_f1(\n",
    "#     pd.DataFrame({'gesture': le.classes_[true_val_int]}),\n",
    "#     pd.DataFrame({'gesture': le.classes_[preds_val]})\n",
    "# )\n",
    "# print(\"Hold-out H-F1 =\", round(h_f1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b429f",
   "metadata": {},
   "source": [
    " |- Model Type\t|Strength|\tEasy to Try?|\n",
    " |------|-------------|---------|\n",
    " |- TCN\tFast,| interpretable\t|✅✅✅\n",
    " |- Transformer |Encoder\tGlobal temporal modeling\t|✅✅\n",
    " |- CNN + Transformer Hybrid\t|Local + global\t|✅✅\n",
    " |- ResNet1D / InceptionTime\t|Robust 1D feature extraction\t|✅✅✅\n",
    " |- BiLSTM + Attention\t|Sequence modeling (non-parallel)\t|✅✅\n",
    " |- ST-GCN\t|Spatial-Temporal & structured\t|❌ (if no graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
