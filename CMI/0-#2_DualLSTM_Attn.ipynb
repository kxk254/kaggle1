{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c3ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json, joblib, re\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\n",
    "\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import gc  # garbage collection\n",
    "import psutil\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.interpolate import CubicSpline, interp1d, PchipInterpolator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f820e34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ imports ready · torch 2.7.1+cu128 device : cuda\n"
     ]
    }
   ],
   "source": [
    "# (Competition metric will only be imported when TRAINing)\n",
    "TRAIN = True                     # ← set to True when you want to train\n",
    "\n",
    "class config:\n",
    "    AMP = False\n",
    "    BATCH_SIZE_TRAIN = 128 #32\n",
    "    BATCH_SIZE_VALID = 128 #32\n",
    "    DEBUG = False\n",
    "    EPOCHS = 150  #30\n",
    "    FOLDS = 5\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    LEARNING_RATE = 2e-3    #vTypical values: 1e-4 to 1e-2.  best 2e-3: .77  1e-3: .77  9e-4:.76  5e-4:.74\n",
    "    MAX_GRAD_NORM = 1e7\n",
    "    WD = 1e-2   #1e-4 (or try higher values like 1e-3 or 5e-4  5e-3:.77  1e-2: .7958\n",
    "    NUM_WORKERS = 0 # multiprocessing.cpu_count()\n",
    "    PRINT_FREQ = 20\n",
    "    SEED = 20\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    PAD_PERCENTILE = 95\n",
    "    SEQUENCE_LENGTH = 150\n",
    "\n",
    "class paths:\n",
    "    BASE_DIR = Path(\"C:/Users/konno/SynologyDrive/datasciense/projects_foler/1_kaggle/CMI/cmi-detect-behavior-with-sensor-data\")\n",
    "    \n",
    "    OUTPUT_DIR = BASE_DIR / \"output-02-wavenet\"\n",
    "    TEST_CSV = BASE_DIR / \"test.csv\"\n",
    "    TEST_DEMOGRAPHICS = BASE_DIR / \"test_demographics.csv\"\n",
    "    TRAIN_CSV = BASE_DIR / \"train.csv\"\n",
    "    TRAIN_DEMOGRAPHICS = BASE_DIR / \"train_demographics.csv\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"▶ imports ready · torch\", torch.__version__, \"device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a92808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b94e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, alpha=0.2):\n",
    "        \"\"\"\n",
    "        X: np.array or torch.Tensor of shape (N, )\n",
    "        y: np.array or torch.Tensor if shaoe (N, )\n",
    "        alpha: Beta distribution parameter for mixup\n",
    "        \"\"\"\n",
    "        self.X = torch.tensor(X, dtype=torch.float32) if isinstance(X, np.ndarray) else X\n",
    "        self.y = torch.tensor(y, dtype=torch.float32) if isinstance(y, np.ndarray) else y\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1, y1 = self.X[idx], self.y[idx]\n",
    "        \n",
    "        # Create shuffle tensor\n",
    "        shuffle_index = np.random.randint(0, len(self.X))\n",
    "        x2, y2 = self.X[shuffle_index], self.y[shuffle_index]       \n",
    "\n",
    "        # Mix\n",
    "        weight = np.random.beta(self.alpha, self.alpha)\n",
    "        x_mix = x1 * weight + x2 * (1 - weight)\n",
    "        y_mix = y1 * weight + y2 * (1 - weight)\n",
    "\n",
    "        return x_mix, y_mix\n",
    "    \n",
    "# train_dataset = MixupDataset(config, df_train, X_tr, y_tr, y_soft_tr)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE_TRAIN, shuffle=True)\n",
    "# val_dataset = CustomDataset(config, df_train, X_val, y_val, y_soft_val)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE_VALID, shuffle=True)\n",
    "\n",
    "def pad_or_truncate(seq, max_len, mode=TRAIN, pad_value=0.0, dtype=np.float32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pads or truncates a sequence to a fixed length.\n",
    "\n",
    "    Parameters:\n",
    "    - seq: np.ndarray of shape (L, D)\n",
    "    - max_len: int, desired sequence length\n",
    "    - mode: bool, True = random pad, False = regular pad\n",
    "    - pad_value: float or int, value to use for padding\n",
    "    - dtype: np.dtype, dtype for the output array\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray of shape (max_len, D)\n",
    "    \"\"\"\n",
    "    # print(\"sequence shape\", seq.shape)\n",
    "    L, D = seq.shape\n",
    "    # print(\"mode = \", mode)\n",
    "\n",
    "    if L > max_len:\n",
    "        return seq[:max_len] # truncate if too long\n",
    "\n",
    "    elif L < max_len:\n",
    "        total_padding = max_len - L\n",
    "        \n",
    "        if mode:\n",
    "            pad_start = np.random.randint(0, total_padding + 1)\n",
    "            pad_end = total_padding - pad_start\n",
    "            \n",
    "        else:\n",
    "            pad_start = 0\n",
    "            pad_end = total_padding\n",
    "\n",
    "        start_padding = np.full((pad_start, D), pad_value, dtype=dtype)\n",
    "        end_padding = np.full((pad_end, D), pad_value, dtype=dtype)\n",
    "        padded = np.vstack((start_padding, seq, end_padding))\n",
    "        # print(\"padded shape\", padded.shape)\n",
    "        return padded\n",
    "\n",
    "    else:\n",
    "        return seq.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce8f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    \n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "             \n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "\n",
    "            # Calculate the relative rotation\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            \n",
    "            # Convert delta rotation to angular velocity vector\n",
    "            # The rotation vector (Euler axis * angle) scaled by 1/dt\n",
    "            # is a good approximation for small delta_rot\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            # If quaternion is invalid, angular velocity remains zero\n",
    "            pass\n",
    "            \n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0 # Или np.nan, в зависимости от желаемого поведения\n",
    "            continue\n",
    "        try:\n",
    "            # Преобразование кватернионов в объекты Rotation\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "\n",
    "            # Вычисление углового расстояния: 2 * arccos(|real(p * q*)|)\n",
    "            # где p* - сопряженный кватернион q\n",
    "            # В scipy.spatial.transform.Rotation, r1.inv() * r2 дает относительное вращение.\n",
    "            # Угол этого относительного вращения - это и есть угловое расстояние.\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            \n",
    "            # Угол rotation vector соответствует угловому расстоянию\n",
    "            # Норма rotation vector - это угол в радианах\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 # В случае недействительных кватернионов\n",
    "            pass\n",
    "            \n",
    "    return angular_dist\n",
    "\n",
    "def print_memory():\n",
    "    process = psutil.Process()\n",
    "    print(f\"Memory Usage: {process.memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "def parse_tof_column(col):\n",
    "    # Match patterns like 'tof_1_v42' or 'tof_1_v42_norm'\n",
    "    match = re.match(r\"tof_(\\d+)_v(\\d+)\", col)\n",
    "    if match:\n",
    "        sensor_num = int(match.group(1))\n",
    "        pixel_num = int(match.group(2))\n",
    "        return (sensor_num, pixel_num)\n",
    "    else:\n",
    "        return (float('inf'), float('inf'))  # put unmatchable columns at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e2aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SEBlock1D(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, time)\n",
    "        b, c, _ = x.size()\n",
    "        y = self.global_avg_pool(x).view(b, c)\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = torch.sigmoid(self.fc2(y)).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ResidualSEBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, pool_size=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.bn1 = nn.GroupNorm(num_groups=8, num_channels=out_channels), #BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.bn2 = nn.GroupNorm(num_groups=8, num_channels=out_channels), #BatchNorm1d(out_channels)\n",
    "        self.se = SEBlock1D(out_channels)\n",
    "        \n",
    "        self.match_channels = None\n",
    "        if in_channels != out_channels:\n",
    "            self.match_channels = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, padding=0, bias=False),\n",
    "                nn.GroupNorm(num_groups=8, num_channels=out_channels), #BatchNorm1d(out_channels)\n",
    "            )\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.gelu(self.bn1(self.conv1(x)))  #relu\n",
    "        out = F.gelu(self.bn2(self.conv2(out)))  #relu\n",
    "        out = self.se(out)\n",
    "\n",
    "        if self.match_channels is not None:\n",
    "            identity = self.match_channels(identity)\n",
    "        \n",
    "        out = F.gelu(out + identity)  #relu\n",
    "        out = self.pool(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.attn_fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, time, features)\n",
    "        # Compute scores with tanh activation\n",
    "        scores = torch.tanh(self.attn_fc(x))  # (batch, time, 1)\n",
    "        scores = scores.squeeze(-1)           # (batch, time)\n",
    "\n",
    "        # Softmax over time dimension to get weights\n",
    "        weights = F.softmax(scores, dim=1)    # (batch, time)\n",
    "        weights = weights.unsqueeze(-1)       # (batch, time, 1)\n",
    "\n",
    "        # Weighted sum of input features over time\n",
    "        context = (x * weights).sum(dim=1)    # (batch, features)\n",
    "        return context\n",
    "    \n",
    "\n",
    "class TwoBranchGestureModel(nn.Module):\n",
    "    def __init__(self, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "        super().__init__()\n",
    "        self.imu_dim = imu_dim\n",
    "        self.tof_dim = tof_dim\n",
    "        \n",
    "        # IMU deep branch\n",
    "        self.imu_branch = nn.Sequential(\n",
    "            ResidualSEBlock1D(imu_dim, 64, kernel_size=3, dropout=0.1),\n",
    "            ResidualSEBlock1D(64, 128, kernel_size=5, dropout=0.1)\n",
    "        )\n",
    "\n",
    "        # TOF Lighter branch\n",
    "        self.tof_branch = nn.Sequential(\n",
    "            nn.Conv1d(tof_dim, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=64),  #BatchNorm1d(64),\n",
    "            nn.GELU(), #ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=64),  #BatchNorm1d(128),\n",
    "            nn.GELU(), #ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(256, 128, batch_first=True, bidirectional=True)\n",
    "        self.gru = nn.GRU(256, 128, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Gaussian noise (manual) and projection\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Dropout(0.09),\n",
    "            nn.Linear(256, 16),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.pre_attn_dropout = nn.Dropout(0.2)\n",
    "        self.attn = AttentionLayer(512)\n",
    "\n",
    "        # Dense layer\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(512, 128, bias=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(), #ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64, bias=False),\n",
    "            nn.GELU(), #ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, n_classes),  # Softmax handled by loss (e.g., CrossEntropyLoss)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        imu = x[:, :, :self.imu_dim].permute(0, 2, 1) #(B, imu_dim, T)\n",
    "        tof = x[: ,:, self.imu_dim:].permute(0, 2, 1) #(B, tof_dim, T)\n",
    "\n",
    "        imu_feat = self.imu_branch(imu)\n",
    "        tof_feat = self.tof_branch(tof)\n",
    "\n",
    "        imu_feat = imu_feat.permute(0, 2, 1)\n",
    "        tof_feat = tof_feat.permute(0, 2, 1)\n",
    "\n",
    "        merged = torch.cat([imu_feat, tof_feat], dim=-1)  #(B, T', 256)\n",
    "\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(merged) * 0.015\n",
    "            merged = merged + noise\n",
    "\n",
    "        xa, _ = self.lstm(merged)\n",
    "        xb, _ = self.gru(merged)\n",
    "        # xc = self.projection(merged)\n",
    "\n",
    "        x_cat = torch.cat([xa, xb], dim=-1)  #(B, T', 512)\n",
    "        x_cat = self.pre_attn_dropout(x_cat)\n",
    "        context = self.attn(x_cat)\n",
    "\n",
    "        return self.mlp(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d061e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####  DATA SAMPLE to delete\n",
    "# df_data = pd.read_csv(paths.TRAIN_CSV, nrows=5000)\n",
    "# df = df_data.fillna(0)\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "# np.save(paths.OUTPUT_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "\n",
    "# # print(df[['gesture_int', 'gesture', 'acc_x']].groupby('gesture').first())\n",
    "\n",
    "# seq_gp = df.groupby('sequence_id') \n",
    "# seq_id, group_df = next(iter(seq_gp))\n",
    "# # print(\"seq id\", seq_id)\n",
    "# # print(\"group df:\", group_df[['gesture_int', 'gesture', 'acc_x']][:3])\n",
    "# # print(\"df\", df[['gesture_int', 'gesture', 'acc_x']][:3])\n",
    "\n",
    "# all_steps_for_scaler_list = []\n",
    "# X_list_unscaled, y_list_int_for_stratify, lens = [], [], [] \n",
    "\n",
    "# for seq_id, seq_df_orig in seq_gp:\n",
    "#     seq_df = seq_df_orig.copy()\n",
    "\n",
    "#     y_list_int_for_stratify.append(seq_df['gesture_int'].iloc[0])\n",
    "\n",
    "# # print(y_list_int_for_stratify[:10])\n",
    "\n",
    "# labels_tensor = torch.tensor(df['gesture_int'].values, dtype=torch.long)\n",
    "# one_hot_tensor = F.one_hot(labels_tensor, num_classes=len(le.classes_))\n",
    "# df['gesture_int_oh'] = one_hot_tensor.numpy().tolist()  # now each cell is a list\n",
    "\n",
    "# subset_df = df[['gesture_int', 'gesture_int_oh']].head(500)\n",
    "# subset_df.to_csv('gesture_with_onehot.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class TwoBranchGestureModel(nn.Module):\n",
    "#     def __init__(self, imu_dim, tof_dim, n_classes, wd=config.WD):\n",
    "#         super().__init__()\n",
    "#         self.imu_dim = imu_dim\n",
    "#         self.tof_dim = tof_dim\n",
    "        \n",
    "#         # IMU deep branch\n",
    "#         self.imu_branch = nn.Sequential(\n",
    "#             ResidualSEBlock1D(imu_dim, 32, kernel_size=3, dropout=0.4),  #64 0.1\n",
    "#             ResidualSEBlock1D(32, 64, kernel_size=5, dropout=0.3)   #64, 128        \n",
    "#             )\n",
    "\n",
    "#         # TOF Lighter branch\n",
    "#         self.tof_branch = nn.Sequential(\n",
    "#             nn.Conv1d(tof_dim, 32, kernel_size=3, padding=1, bias=False),  #64\n",
    "#             nn.BatchNorm1d(32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(2),\n",
    "#             nn.Dropout(0.4), #0.2\n",
    "#             nn.Conv1d(32, 64, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm1d(64),\n",
    "#             nn.MaxPool1d(2),\n",
    "#             nn.Dropout(0.3),  #0.3\n",
    "#         )\n",
    "\n",
    "#         # self.lstm = nn.LSTM(128, 64, batch_first=True, bidirectional=True)  #256, 128\n",
    "#         self.gru = nn.GRU(128, 64, batch_first=True, bidirectional=True)\n",
    "\n",
    "#         # Gaussian noise (manual) and projection\n",
    "#         # self.projection = nn.Sequential(\n",
    "#         #     nn.Dropout(0.1),  #0.09\n",
    "#         #     nn.Linear(128, 16),  #256\n",
    "#         #     nn.ELU()\n",
    "#         # )\n",
    "#         self.projection = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "#         self.pre_attn_dropout = nn.Dropout(0.4)\n",
    "#         self.attn = AttentionLayer(128)  #128*2 + 128*2 + 16\n",
    "\n",
    "#         # Dense layer\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(128, 128, bias=False),   #528 , 256\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(128, 64, bias=False),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "#             nn.Linear(64, n_classes),  # Softmax handled by loss (e.g., CrossEntropyLoss)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         imu = x[:, :, :self.imu_dim].permute(0, 2, 1) #(B, imu_dim, T)\n",
    "#         tof = x[: ,:, self.imu_dim:].permute(0, 2, 1) #(B, tof_dim, T)\n",
    "\n",
    "#         imu_feat = self.imu_branch(imu)\n",
    "#         tof_feat = self.tof_branch(tof)\n",
    "\n",
    "#         imu_feat = imu_feat.permute(0, 2, 1)\n",
    "#         tof_feat = tof_feat.permute(0, 2, 1)\n",
    "\n",
    "#         merged = torch.cat([imu_feat, tof_feat], dim=-1)  #(B, T', 256)\n",
    "\n",
    "#         # xa, _ = self.lstm(merged)\n",
    "#         xb, _ = self.gru(merged)\n",
    "#         # xb = xb.permute(0, 2, 1) \n",
    "#         # xb_pooled = F.adaptive_avg_pool1d(xb, 1).squeeze(-1)  # [B, F]\n",
    "\n",
    "#         xb = self.pre_attn_dropout(xb) # Add dropout before attention\n",
    "#         # xc = self.projection(merged)\n",
    "#         # xc = self.projection(merged.permute(0, 2, 1)).squeeze(-1)  # [B, F]\n",
    "#         # print(f\"xa shape {xa.shape} / xb shape {xb.shape} / xc shape {xc.shape}\")\n",
    "\n",
    "#         attended_output = self.attn(xb) # Assuming attn takes (B, T, F) and outputs (B, F)\n",
    "#         # x_cat = torch.cat([xb, xc], dim=-1)  #(B, T', 512)\n",
    "#         # x_cat = self.pre_attn_dropout(x_cat)\n",
    "#         # x_cat = torch.cat([xb_pooled, xc], dim=-1)  # (B, features, T)\n",
    "#         # context = F.adaptive_avg_pool1d(x_cat, 1).squeeze(-1)  # (B, features)\n",
    "#         # context = self.attn(x_cat)\n",
    "\n",
    "#         return self.mlp(attended_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75992ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ TRAIN MODE – loading dataset …\n",
      "merged df shape : (574945, 348)\n",
      "Memory Usage: 3649.21 MB\n",
      " 0/6 Calculating elbow_to_wrist_cm shoulder_to_wrist_cm adjustment ...\n",
      " 1/6 Calculating base engineered IMU features (magnitude, angle) ...\n",
      " 2/6 Calculating engineered IMU derivatives (jerk, angular velocity) for original acc_mag ...\n",
      " 3/6 Removing gravity and calculating linear acceleration features...\n",
      " 4/6 Calculating angular velocity from quaternion derivatives...\n",
      " 5/6 Calculating angular distance between successive quaternions...\n",
      "Memory Usage: 5366.58 MB\n",
      " 6/6 Calculating imu_cols_base ...\n",
      "length of imu_cols : 35 Obtaining tof columns ......\n",
      "tof_columns length : 320\n",
      "✅ Preprocessing done.\n",
      "Memory Usage: 5366.59 MB\n"
     ]
    }
   ],
   "source": [
    "### DATA CREATION and PRE PROCESSING\n",
    "\n",
    "print(\"▶ TRAIN MODE – loading dataset …\")\n",
    "\n",
    "df_data = pd.read_csv(paths.TRAIN_CSV)\n",
    "df_data = df_data.fillna(0)\n",
    "\n",
    "train_dem_df = pd.read_csv(paths.TRAIN_DEMOGRAPHICS)\n",
    "df = pd.merge(df_data.copy(), train_dem_df, on='subject', how='left')\n",
    "print(\"merged df shape :\", df.shape)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "np.save(paths.OUTPUT_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "gesture_classes = le.classes_\n",
    "\n",
    "print_memory()\n",
    "\n",
    "print(\" 0/6 Calculating elbow_to_wrist_cm shoulder_to_wrist_cm adjustment ...\")\n",
    "\n",
    "df[\"acc_x_norm_ew\"] = df[\"acc_x\"] / df[\"elbow_to_wrist_cm\"]\n",
    "df[\"acc_y_norm_ew\"] = df[\"acc_y\"] / df[\"elbow_to_wrist_cm\"]\n",
    "df[\"acc_z_norm_ew\"] = df[\"acc_z\"] / df[\"elbow_to_wrist_cm\"]\n",
    "\n",
    "df[\"acc_x_norm_sw\"] = df[\"acc_x\"] / df[\"shoulder_to_wrist_cm\"]\n",
    "df[\"acc_y_norm_sw\"] = df[\"acc_y\"] / df[\"shoulder_to_wrist_cm\"]\n",
    "df[\"acc_z_norm_sw\"] = df[\"acc_z\"] / df[\"shoulder_to_wrist_cm\"]\n",
    "\n",
    "print(\" 1/6 Calculating base engineered IMU features (magnitude, angle) ...\")\n",
    "\n",
    "df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "df['rot_angle'] = 2* np.arccos(df['rot_w'].clip(-1, 1))\n",
    "\n",
    "print(\" 2/6 Calculating engineered IMU derivatives (jerk, angular velocity) for original acc_mag ...\")\n",
    "\n",
    "df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "\n",
    "print(\" 3/6 Removing gravity and calculating linear acceleration features...\")\n",
    "\n",
    "linear_accel_list = []\n",
    "for _, group in df.groupby('sequence_id'):\n",
    "    acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n",
    "    rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "    linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
    "    linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n",
    "\n",
    "df_linear_accel = pd.concat(linear_accel_list)\n",
    "df = pd.concat([df, df_linear_accel], axis=1)\n",
    "del df_linear_accel, linear_accel_list  # Memory Management\n",
    "gc.collect()  # Memory Management\n",
    "\n",
    "df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "\n",
    "print(\" 4/6 Calculating angular velocity from quaternion derivatives...\")\n",
    "angular_vel_list = []\n",
    "for _, group in df.groupby('sequence_id'):\n",
    "    rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "    angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
    "    angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n",
    "\n",
    "df_angular_vel = pd.concat(angular_vel_list)\n",
    "df = pd.concat([df, df_angular_vel], axis=1)\n",
    "del angular_vel_list, df_angular_vel # Memory Management\n",
    "gc.collect() # Memory Management\n",
    "\n",
    "print(\" 5/6 Calculating angular distance between successive quaternions...\")\n",
    "angular_distance_list = []\n",
    "for _, group in df.groupby('sequence_id'):\n",
    "    rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "    angular_dist_group = calculate_angular_distance(rot_data_group)\n",
    "    angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n",
    "\n",
    "df_angular_distance = pd.concat(angular_distance_list)\n",
    "df = pd.concat([df, df_angular_distance], axis=1)\n",
    "del angular_distance_list, df_angular_distance # Memory Management\n",
    "gc.collect() # Memory Management\n",
    "\n",
    "print_memory()\n",
    "\n",
    "meta_cols = { } # This was an empty dict in your provided code, keeping it as is.\n",
    "\n",
    "print(\" 6/6 Calculating imu_cols_base ...\")\n",
    "imu_cols_orig = ['acc_x', 'acc_y', 'acc_z',\n",
    "            'rot_w', 'rot_x', 'rot_y', 'rot_z',\n",
    "            'thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5']\n",
    "\n",
    "imu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n",
    "imu_cols_base.extend([c for c in df.columns if c.startswith('rot_') and c not in ['rot_angle', 'rot_angle_vel']])\n",
    "\n",
    "imu_engineered_features = [\n",
    "    'acc_x_norm_ew', 'acc_y_norm_ew', 'acc_z_norm_ew',  # new from demographics\n",
    "    'acc_x_norm_sw', 'acc_y_norm_sw', 'acc_z_norm_sw',  # new from demographics\n",
    "    'acc_mag', 'rot_angle',\n",
    "    'acc_mag_jerk', 'rot_angle_vel',\n",
    "    'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "    'angular_vel_x', 'angular_vel_y', 'angular_vel_z', # Existing new features\n",
    "    'angular_distance' # Added new feature\n",
    "]\n",
    "\n",
    "dem_features = [\n",
    "    'adult_child', 'age',\n",
    "    'sex', 'handedness',\n",
    "]\n",
    "\n",
    "imu_cols = list(dict.fromkeys(imu_cols_orig + imu_cols_base + imu_engineered_features + dem_features))  # Remove dups\n",
    "\n",
    "print(\"length of imu_cols :\", len(imu_cols), \"Obtaining tof columns ......\")\n",
    "\n",
    "tof_columns = [col for col in df.columns if col.startswith(\"tof_\")]\n",
    "tof_columns = sorted(tof_columns, key=parse_tof_column)\n",
    "\n",
    "sequence_ids = df[\"sequence_id\"].unique()\n",
    "\n",
    "print(\"tof_columns length :\", len(tof_columns))\n",
    "\n",
    "del imu_cols_orig, imu_cols_base, imu_engineered_features, dem_features # Memory Management\n",
    "gc.collect() # Memory Management\n",
    "\n",
    "print(\"✅ Preprocessing done.\")\n",
    "print_memory()\n",
    "\n",
    "# thm_cols_original = [c for c in df.columns if c.startswith('thm_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbb0061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQUENCE_LENGTH : 127\n",
      "X shape torch.Size([8151, 127, 355]) | y shape torch.Size([8151, 18])\n",
      "length of imu 35\n",
      "length of tof 320\n",
      "Memory Usage: 9749.54 MB\n"
     ]
    }
   ],
   "source": [
    "### DATA CONFIGURATION\n",
    "\n",
    "# Estimate the max length\n",
    "sequence_lengths = df.groupby('sequence_id').size().values  # length of each sequence\n",
    "SEQUENCE_LENGTH = int(np.percentile(sequence_lengths, 95))\n",
    "print(\"SEQUENCE_LENGTH :\", SEQUENCE_LENGTH)\n",
    "\n",
    "X_2dim = df[imu_cols + tof_columns]\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for seq_id, group in df.groupby('sequence_id', sort=False):\n",
    "    X_seq = group[imu_cols + tof_columns].values.astype(np.float32)\n",
    "    X_list.append(X_seq)\n",
    "    y_list.append(group['gesture_int'].iloc[0])\n",
    "\n",
    "X_padded = np.stack([pad_or_truncate(seq, SEQUENCE_LENGTH) for seq in X_list])\n",
    "X = torch.tensor(X_padded, dtype=torch.float32)\n",
    "y = F.one_hot(torch.tensor(np.array(y_list)), num_classes=len(le.classes_)).float()\n",
    "print(f\"X shape {X.shape} | y shape {y.shape}\")\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,  # 20% validation\n",
    "    random_state=42,\n",
    "    stratify=df.groupby(\"sequence_id\")[\"gesture\"].first()  # keeps gesture label distribution balanced\n",
    ")\n",
    "\n",
    "## Sanity Check\n",
    "for i, (seq_id, group) in enumerate(df.groupby('sequence_id', sort=False)):\n",
    "    assert y_list[i] == group['gesture_int'].iloc[0]\n",
    "print(\"length of imu\", len(imu_cols))\n",
    "print(\"length of tof\", len(tof_columns))\n",
    "\n",
    "train_dataset = MotionDataset(X_tr, y_tr, alpha=0.2)\n",
    "val_dataset   = MotionDataset(X_val, y_val, alpha=0.2)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE_TRAIN, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=config.BATCH_SIZE_VALID, shuffle=False, num_workers=0)\n",
    "\n",
    "del X_list, y_list, X_padded\n",
    "\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e9db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = TwoBranchGestureModel(\n",
    "    imu_dim=len(imu_cols),         # channels per node (ToF + IMU)\n",
    "    tof_dim=len(tof_columns),         # channels per node (ToF + IMU)\n",
    "    n_classes=len(df[\"gesture\"].unique()),  # e.g., 20\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c34d487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ training started .....\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n",
      "(18,)\n",
      "▶️ Setting scheduler  .....\n",
      "✅ Epoch starts .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:04, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 2.5386 | Train Acc: 0.2259 | lr = 0.002000\n",
      "Epoch 0 | Val Acc: 0.3495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 16.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 2.0398 | Train Acc: 0.3956 | lr = 0.002000\n",
      "Epoch 1 | Val Acc: 0.3998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 1.8911 | Train Acc: 0.4656 | lr = 0.002000\n",
      "Epoch 2 | Val Acc: 0.4838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 16.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 1.8404 | Train Acc: 0.5018 | lr = 0.002000\n",
      "Epoch 3 | Val Acc: 0.5285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 16.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 1.8073 | Train Acc: 0.5491 | lr = 0.002000\n",
      "Epoch 4 | Val Acc: 0.5782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:02, 17.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 1.6903 | Train Acc: 0.5753 | lr = 0.002000\n",
      "Epoch 5 | Val Acc: 0.5947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 16.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 1.6465 | Train Acc: 0.5974 | lr = 0.002000\n",
      "Epoch 6 | Val Acc: 0.5788\n",
      "patience_counter : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:02, 17.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 1.6271 | Train Acc: 0.6126 | lr = 0.002000\n",
      "Epoch 7 | Val Acc: 0.5849\n",
      "patience_counter : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:02, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 1.5964 | Train Acc: 0.6235 | lr = 0.002000\n",
      "Epoch 8 | Val Acc: 0.6119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:02, 17.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 1.5795 | Train Acc: 0.6399 | lr = 0.002000\n",
      "Epoch 9 | Val Acc: 0.6186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 16.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 1.5579 | Train Acc: 0.6500 | lr = 0.002000\n",
      "Epoch 10 | Val Acc: 0.6358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:02, 17.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 1.5276 | Train Acc: 0.6563 | lr = 0.002000\n",
      "Epoch 11 | Val Acc: 0.6156\n",
      "patience_counter : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:02, 17.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 1.5244 | Train Acc: 0.6675 | lr = 0.002000\n",
      "Epoch 12 | Val Acc: 0.6303\n",
      "patience_counter : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:02, 17.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 1.5303 | Train Acc: 0.6706 | lr = 0.002000\n",
      "Epoch 13 | Val Acc: 0.5181\n",
      "patience_counter : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:02, 17.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 1.5073 | Train Acc: 0.6845 | lr = 0.002000\n",
      "Epoch 14 | Val Acc: 0.6291\n",
      "patience_counter : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 15.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 1.4318 | Train Acc: 0.7112 | lr = 0.001000\n",
      "Epoch 15 | Val Acc: 0.6910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 1.4012 | Train Acc: 0.7316 | lr = 0.001000\n",
      "Epoch 16 | Val Acc: 0.6511\n",
      "patience_counter : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss: 1.3944 | Train Acc: 0.7456 | lr = 0.001000\n",
      "Epoch 17 | Val Acc: 0.6732\n",
      "patience_counter : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 15.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 1.3758 | Train Acc: 0.7439 | lr = 0.001000\n",
      "Epoch 18 | Val Acc: 0.6934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 14.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss: 1.3680 | Train Acc: 0.7495 | lr = 0.001000\n",
      "Epoch 19 | Val Acc: 0.6836\n",
      "patience_counter : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss: 1.3471 | Train Acc: 0.7569 | lr = 0.001000\n",
      "Epoch 20 | Val Acc: 0.7247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 14.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Train Loss: 1.3263 | Train Acc: 0.7660 | lr = 0.001000\n",
      "Epoch 21 | Val Acc: 0.7143\n",
      "patience_counter : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 14.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Train Loss: 1.3401 | Train Acc: 0.7729 | lr = 0.001000\n",
      "Epoch 22 | Val Acc: 0.6830\n",
      "patience_counter : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Train Loss: 1.3305 | Train Acc: 0.7801 | lr = 0.001000\n",
      "Epoch 23 | Val Acc: 0.6990\n",
      "patience_counter : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 14.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Train Loss: 1.3259 | Train Acc: 0.7775 | lr = 0.001000\n",
      "Epoch 24 | Val Acc: 0.6996\n",
      "patience_counter : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:04, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Train Loss: 1.3034 | Train Acc: 0.7923 | lr = 0.000500\n",
      "Epoch 25 | Val Acc: 0.7376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:04, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | Train Loss: 1.2713 | Train Acc: 0.8071 | lr = 0.000500\n",
      "Epoch 26 | Val Acc: 0.7069\n",
      "patience_counter : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 14.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Train Loss: 1.2592 | Train Acc: 0.8137 | lr = 0.000500\n",
      "Epoch 27 | Val Acc: 0.7167\n",
      "patience_counter : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 15.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 | Train Loss: 1.2577 | Train Acc: 0.8163 | lr = 0.000500\n",
      "Epoch 28 | Val Acc: 0.7278\n",
      "patience_counter : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 16.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 | Train Loss: 1.2398 | Train Acc: 0.8173 | lr = 0.000500\n",
      "Epoch 29 | Val Acc: 0.7051\n",
      "patience_counter : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 16.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | Train Loss: 1.2252 | Train Acc: 0.8347 | lr = 0.000250\n",
      "Epoch 30 | Val Acc: 0.7370\n",
      "patience_counter : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 14.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 | Train Loss: 1.1966 | Train Acc: 0.8339 | lr = 0.000250\n",
      "Epoch 31 | Val Acc: 0.7357\n",
      "patience_counter : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 15.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 | Train Loss: 1.2008 | Train Acc: 0.8344 | lr = 0.000250\n",
      "Epoch 32 | Val Acc: 0.7198\n",
      "patience_counter : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 14.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 | Train Loss: 1.2007 | Train Acc: 0.8433 | lr = 0.000250\n",
      "Epoch 33 | Val Acc: 0.7357\n",
      "patience_counter : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 | Train Loss: 1.1873 | Train Acc: 0.8489 | lr = 0.000125\n",
      "Epoch 34 | Val Acc: 0.7253\n",
      "patience_counter : 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 15.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 | Train Loss: 1.1800 | Train Acc: 0.8491 | lr = 0.000125\n",
      "Epoch 35 | Val Acc: 0.7357\n",
      "patience_counter : 10\n",
      "Early stopping triggered.\n",
      "Best Model : 0.7376\n"
     ]
    }
   ],
   "source": [
    "print(\"⏩ training started .....\")\n",
    "\n",
    "sequence_labels = df.groupby('sequence_id').first()['gesture_int'].values\n",
    "print(np.unique(sequence_labels)) \n",
    "cw_vals = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(sequence_labels),\n",
    "    y=torch.argmax(y, dim=1).numpy()\n",
    "    )\n",
    "print(cw_vals.shape)  # should be (num_classes,)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WD)\n",
    "weights_tensor = torch.tensor(cw_vals, dtype=torch.float32).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights_tensor, label_smoothing=0.1)\n",
    "# loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "print(\"▶️ Setting scheduler  .....\")\n",
    "steps = []\n",
    "lrs = []\n",
    "best_val_acc = 0\n",
    "patience, patience_counter = 10, 0\n",
    "EPOCHS = config.EPOCHS\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    # verbose=True, \n",
    "    threshold=0.0001,\n",
    "    threshold_mode='rel',\n",
    ")\n",
    "\n",
    "print(\"✅ Epoch starts .....\")\n",
    "import itertools\n",
    "\n",
    "max_batches = 5\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0         # <-- reset here\n",
    "    total = 0           # <-- reset here\n",
    "    # for batch_idx, batch in tqdm.tqdm(enumerate(itertools.islice(train_loader, max_batches))):        \n",
    "    for batch_idx, batch in tqdm.tqdm(enumerate(train_loader)):\n",
    "        xb, yb = batch[0].to(device), batch[1].to(device)\n",
    "        # if batch_idx == 0:\n",
    "        #     print(f\"Batch {batch_idx}: x_imu shape {x_imu.shape}, x_tof shape {x_tof.shape}\")\n",
    "\n",
    "        # if batch_idx == 0:\n",
    "        #     print(f\"Batch {batch_idx}: x_imu shape {xb.shape}, x_tof shape {yb.shape}\")\n",
    "        optimizer.zero_grad()        \n",
    "        logits = model(xb)\n",
    "        # print(\"logits.shape:\", logits.shape)\n",
    "        # print(\"yb_indices.shape:\", yb.shape)\n",
    "        yb_indices = yb.argmax(dim=1)\n",
    "        loss = loss_fn(logits, yb_indices)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # optional\n",
    "        optimizer.step()        \n",
    "\n",
    "        total_loss += loss.item()\n",
    "        lrs.append(scheduler.get_last_lr()[0])\n",
    "        steps.append(epoch * config.BATCH_SIZE_TRAIN + batch_idx)\n",
    "        \n",
    "        logits_arg = logits.argmax(dim=1)\n",
    "        correct += (logits_arg == yb_indices).sum().item()\n",
    "        total += yb_indices.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch} | Train Loss: {total_loss / len(train_loader):.4f} | Train Acc: {train_acc:.4f} | lr = {current_lr:.6f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            xb, yb = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            x_preds = model(xb)\n",
    "            logits = x_preds.argmax(dim=1)\n",
    "            true_labels = yb.argmax(1) if yb.ndim > 1 else yb  #.argmax(1)  val_loader comes from a standard dataset with \"y\" as class index (long), you don’t need argmax.\n",
    "            correct += (logits == true_labels).sum().item()\n",
    "            total += true_labels.size(0)\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), paths.OUTPUT_DIR / \"best_model.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(\"patience_counter :\", patience_counter)\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "print(f\"Best Model : {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840200d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2138e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c677c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
